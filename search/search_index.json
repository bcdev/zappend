{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#zappend-documentation","title":"zappend Documentation","text":"<p><code>zappend</code> is a tool written in Python that is used for robustly creating and  updating Zarr datacubes from smaller dataset slices. It is built on top of the  awesome Python packages xarray and zarr.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>The objective of <code>zappend</code> is enabling geodata scientists and developers to  robustly create large data cubes. The tool performs transaction-based dataset  appends to existing data cubes in the  Zarr format. If an error  occurs during an append step \u2014 typically due to I/O problems or out-of-memory  conditions \u2014 <code>zappend</code> will automatically roll back the operation, ensuring that  the existing data cube maintains its structural integrity. The design drivers  behind zappend are first ease of use and secondly, high configurability  regarding filesystems, data source types, data cube outline and encoding. </p>"},{"location":"#features","title":"Features","text":"<p>The <code>zappend</code> tool provides the following features:</p> <ul> <li>Locking: While the target dataset is being modified, a file lock is    created, effectively preventing concurrent dataset modifications.</li> <li>Transaction-based dataset appends: On failure during an append step,    the transaction is rolled back, so that the target dataset remains valid and    preserves its integrity.</li> <li>Filesystem transparency: The target dataset may be generated and updated    in any writable filesystems supported by the    fsspec package.    The same holds for the slice datasets to be appended.</li> <li>Dataset polling: The tool can be configured to wait for slice datasets to    become available. </li> <li>Dynamic attributes: Use syntax <code>{{ expression }}</code> to update the target    dataset with dynamically computed attribute values. </li> <li>CLI and Python API: The tool can be used in a shell using the    <code>zappend</code> command or from Python. When used from Python using the    <code>zappend()</code> function, slice datasets can be passed as local file    paths, URIs, as datasets of type    xarray.Dataset, or as custom   slice sources.</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>At its core, <code>zappend</code> calls the to_zarr() method of xarray.Dataset  for each dataset slice it receives and either creates the target dataset if it does  not exist yet or updates it with the current slice, if it already exists.</p> <p>If there is no target dataset yet, <code>zappend</code> does the following:</p> <ul> <li>get target dataset outline and encoding from configuration and first slice dataset;</li> <li>tailor first slice dataset according to target dataset outline;</li> <li>write target from first slice using target dataset encoding.</li> </ul> <p>If target dataset exists, then <code>zappend</code> will:</p> <ul> <li>get target dataset outline and encoding configuration and existing target dataset;</li> <li>for each subsequent slice dataset:<ul> <li>verify target and slice dataset are compatible;</li> <li>tailor slice dataset according to target dataset outline;</li> <li>remove all metadata including encoding and attributes from slice dataset;</li> <li>concatenate the \"naked\" slice dataset with the target dataset.</li> </ul> </li> </ul>"},{"location":"about/","title":"About zappend","text":""},{"location":"about/#changelog","title":"Changelog","text":"<p>You can find the complete <code>zappend</code> changelog  here. </p>"},{"location":"about/#reporting","title":"Reporting","text":"<p>If you have suggestions, ideas, feature requests, or if you have identified a malfunction or error, then please  post an issue. </p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>The <code>zappend</code> project welcomes contributions of any form as long as you respect our  code of conduct and follow our  contribution guide.</p> <p>If you'd like to submit code or documentation changes, we ask you to provide a  pull request (PR)  here.  For code and configuration changes, your PR must be linked to a  corresponding issue. </p>"},{"location":"about/#development","title":"Development","text":"<p>Setup development environment:</p> <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-docs.txt\n</code></pre>"},{"location":"about/#testing-and-coverage","title":"Testing and Coverage","text":"<p><code>zappend</code> uses pytest for unit-level testing  and code coverage analysis.</p> <pre><code>pytest --cov=zappend tests\n</code></pre>"},{"location":"about/#code-style","title":"Code Style","text":"<p><code>zappend</code> source code is formatted using the black tool.</p> <pre><code>black zappend\n</code></pre>"},{"location":"about/#documentation","title":"Documentation","text":"<p><code>zappend</code> documentation is built using the mkdocs tool.</p> <pre><code>pip install -r requirements-doc.txt\n\nmkdocs build\nmkdocs serve\nmkdocs gh-deploy\n</code></pre> <p>If the configuration JSON schema in <code>zappend/config/schema.py</code> changes then the configuration reference documentation <code>docs/config.md</code> must be  regenerated:</p> <pre><code>zappend --help-config md &gt; docs/config.md\n</code></pre>"},{"location":"about/#license","title":"License","text":"<p><code>zappend</code> is open source made available under the terms and conditions of the  MIT License.</p> <p>Copyright \u00a9 2024 Brockmann Consult Development</p>"},{"location":"api/","title":"Python API reference","text":"<p>All described objects can be imported from the <code>zappend.api</code> module.</p>"},{"location":"api/#function-zappend","title":"Function <code>zappend()</code>","text":""},{"location":"api/#zappend.api.zappend","title":"<code>zappend.api.zappend(slices, config=None, **kwargs)</code>","text":"<p>Robustly create or update a Zarr dataset from dataset slices.</p> <p>The <code>zappend</code> function concatenates the dataset slices from given <code>slices</code> along a given append dimension, e.g., <code>\"time\"</code> (the default) for geospatial satellite observations. Each append step is atomic, that is, the append operation is a transaction that can be rolled back, in case the append operation fails. This ensures integrity of the  target data cube <code>target_dir</code> given in <code>config</code> or <code>kwargs</code>.</p> <p>Each slice item in <code>slices</code> provides a slice dataset to be appended. The interpretation of a given slice item depends on whether a slice source is configured or not (setting <code>slice_source</code>).</p> <p>If no slice source is configured, a slice item must be an object of type <code>str</code>, <code>FileObj</code>, <code>xarray.Dataset</code>, or <code>SliceSource</code>. If <code>str</code> or <code>FileObj</code> are used, they are interpreted as local dataset path or dataset URI. If a URI is used, protocol-specific parameters apply, given by the configuration parameter <code>slice_storage_options</code>.</p> <p>If a slice source is configured, a slice item represents the argument(s) passed to that slice source. Multiple positional arguments can be passed as <code>list</code>, multiple keyword arguments as <code>dict</code>, and both as a <code>tuple</code> of <code>list</code> and <code>dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[Any]</code> <p>An iterable that yields slice items.</p> required <code>config</code> <code>ConfigLike</code> <p>Processor configuration. May be a file path or URI, a <code>dict</code>, <code>None</code>, or a sequence of the aforementioned. If a sequence is used, subsequent configurations are incremental to the previous ones.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional configuration parameters. Can be used to pass or override configuration values in config.</p> <code>{}</code>"},{"location":"api/#class-slicesource","title":"Class <code>SliceSource</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Slice source interface definition.</p> <p>A slice source is a closable source for a slice dataset.</p> <p>A slice source is intended to be implemented by users. An implementation must provide the methods get_dataset() and close().</p> <p>If your slice source class requires the processing context, your class constructor may define a <code>ctx: Context</code> as 1st positional argument or as keyword argument.</p>"},{"location":"api/#zappend.api.SliceSource.close","title":"<code>close()</code>","text":"<p>Close this slice source. This should include cleaning up of any temporary resources.</p> <p>This method is not intended to be called directly and is called exactly once for each instance of this class.</p>"},{"location":"api/#zappend.api.SliceSource.dispose","title":"<code>dispose()</code>","text":"<p>Deprecated since version 0.6.0, override close() instead.</p>"},{"location":"api/#zappend.api.SliceSource.get_dataset","title":"<code>get_dataset()</code>  <code>abstractmethod</code>","text":"<p>Open this slice source, do some processing and return a dataset of type xarray.Dataset as result.</p> <p>This method is not intended to be called directly and is called exactly once for each instance of this class.</p> <p>It should return a dataset that is compatible with target dataset:</p> <ul> <li>slice must have same fixed dimensions;</li> <li>append dimension must exist in slice.</li> </ul> <p>Returns:</p> Type Description <code>Dataset</code> <p>A slice dataset.</p>"},{"location":"api/#class-context","title":"Class <code>Context</code>","text":"<p>Provides access to configuration values and values derived from it.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any] | Config</code> <p>A validated configuration dictionary or a <code>Config</code> instance.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target_dir</code> is missing in the configuration.</p>"},{"location":"api/#zappend.api.Context.config","title":"<code>config: Config</code>  <code>property</code>","text":"<p>The processor configuration.</p>"},{"location":"api/#zappend.api.Context.last_append_label","title":"<code>last_append_label: Any | None</code>  <code>property</code>","text":"<p>The last label found in the coordinate variable that corresponds to the append dimension. Its value is <code>None</code> if no such variable exists or the variable is empty or if <code>config.append_step</code> is <code>None</code>.</p>"},{"location":"api/#zappend.api.Context.target_metadata","title":"<code>target_metadata: DatasetMetadata | None</code>  <code>property</code> <code>writable</code>","text":"<p>The metadata for the target dataset. May be <code>None</code> while the target dataset hasn't been created yet. Will be set, once the target dataset has been created from the first slice dataset.</p>"},{"location":"api/#zappend.api.Context.get_dataset_metadata","title":"<code>get_dataset_metadata(dataset)</code>","text":"<p>Get the dataset metadata from configuration and the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset</p> required <p>Returns:</p> Type Description <code>DatasetMetadata</code> <p>The dataset metadata</p>"},{"location":"api/#class-config","title":"Class <code>Config</code>","text":"<p>Provides access to configuration values and values derived from it.</p> <p>Parameters:</p> Name Type Description Default <code>config_dict</code> <code>Dict[str, Any]</code> <p>A validated configuration dictionary.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target_dir</code> is missing in the configuration.</p>"},{"location":"api/#zappend.api.Config.append_dim","title":"<code>append_dim: str</code>  <code>property</code>","text":"<p>The name of the append dimension along which slice datasets will be concatenated. Defaults to <code>\"time\"</code>.</p>"},{"location":"api/#zappend.api.Config.append_step","title":"<code>append_step: int | float | str | None</code>  <code>property</code>","text":"<p>The enforced step size in the append dimension between two slices. Defaults to <code>None</code>.</p>"},{"location":"api/#zappend.api.Config.attrs","title":"<code>attrs: dict[str, Any]</code>  <code>property</code>","text":"<p>Global dataset attributes. May include dynamically computed placeholders if the form <code>{{ expression }}</code>.</p>"},{"location":"api/#zappend.api.Config.attrs_update_mode","title":"<code>attrs_update_mode: Literal['keep'] | Literal['replace'] | Literal['update']</code>  <code>property</code>","text":"<p>The mode used to deal with global slice dataset attributes. One of <code>\"keep\"</code>, <code>\"replace\"</code>, <code>\"update\"</code>.</p>"},{"location":"api/#zappend.api.Config.disable_rollback","title":"<code>disable_rollback: bool</code>  <code>property</code>","text":"<p>Whether to disable transaction rollbacks.</p>"},{"location":"api/#zappend.api.Config.dry_run","title":"<code>dry_run: bool</code>  <code>property</code>","text":"<p>Whether to run in dry mode.</p>"},{"location":"api/#zappend.api.Config.excluded_variables","title":"<code>excluded_variables: list[str]</code>  <code>property</code>","text":"<p>Names of excluded variables.</p>"},{"location":"api/#zappend.api.Config.extra","title":"<code>extra: dict[str, Any]</code>  <code>property</code>","text":"<p>Extra settings. Intended use is by a <code>slice_source</code> that expects an argument named <code>ctx</code> to access the extra settings and other configuration.</p>"},{"location":"api/#zappend.api.Config.force_new","title":"<code>force_new: bool</code>  <code>property</code>","text":"<p>If set, an existing target dataset will be deleted.</p>"},{"location":"api/#zappend.api.Config.included_variables","title":"<code>included_variables: list[str]</code>  <code>property</code>","text":"<p>Names of included variables.</p>"},{"location":"api/#zappend.api.Config.logging","title":"<code>logging: dict[str, Any] | str | bool | None</code>  <code>property</code>","text":"<p>Logging configuration.</p>"},{"location":"api/#zappend.api.Config.permit_eval","title":"<code>permit_eval: bool</code>  <code>property</code>","text":"<p>Check if dynamically computed values in dataset attributes <code>attrs</code> using the syntax <code>{{ expression }}</code> is permitted. Executing arbitrary Python expressions is a security risk, therefore this must be explicitly enabled.</p>"},{"location":"api/#zappend.api.Config.persist_mem_slices","title":"<code>persist_mem_slices: bool</code>  <code>property</code>","text":"<p>Whether to persist in-memory slice datasets.</p>"},{"location":"api/#zappend.api.Config.profiling","title":"<code>profiling: dict[str, Any] | str | bool | None</code>  <code>property</code>","text":"<p>Profiling configuration.</p>"},{"location":"api/#zappend.api.Config.slice_engine","title":"<code>slice_engine: str | None</code>  <code>property</code>","text":"<p>The configured slice engine to be used if a slice path or URI does not point to a dataset in Zarr format. If defined, it will be passed to the <code>xarray.open_dataset()</code> function.</p>"},{"location":"api/#zappend.api.Config.slice_polling","title":"<code>slice_polling: tuple[float, float] | tuple[None, None]</code>  <code>property</code>","text":"<p>The configured slice dataset polling. If slice polling is enabled, returns tuple (interval, timeout) in seconds, otherwise, return (None, None).</p>"},{"location":"api/#zappend.api.Config.slice_source","title":"<code>slice_source: Callable[[...], Any] | None</code>  <code>property</code>","text":"<p>A class or function that receives a slice item as argument(s) and provides the slice dataset.</p> <ul> <li>If a class is given, it must be derived from <code>zappend.api.SliceSource</code>.</li> <li>If the function is a context manager, it must yield an <code>xarray.Dataset</code>.</li> <li>If a plain function is given, it must return any valid slice item type.</li> </ul> <p>Refer to the user guide for more information.</p>"},{"location":"api/#zappend.api.Config.slice_source_kwargs","title":"<code>slice_source_kwargs: dict[str, Any] | None</code>  <code>property</code>","text":"<p>Extra keyword-arguments passed to a specified <code>slice_source</code> together with each slice item.</p>"},{"location":"api/#zappend.api.Config.slice_storage_options","title":"<code>slice_storage_options: dict[str, Any] | None</code>  <code>property</code>","text":"<p>The configured slice storage options to be used if a slice item is a URI.</p>"},{"location":"api/#zappend.api.Config.target_dir","title":"<code>target_dir: FileObj</code>  <code>property</code>","text":"<p>The configured directory that represents the target datacube in Zarr format.</p>"},{"location":"api/#zappend.api.Config.temp_dir","title":"<code>temp_dir: FileObj</code>  <code>property</code>","text":"<p>The configured directory used for temporary files such as rollback data.</p>"},{"location":"api/#zappend.api.Config.variables","title":"<code>variables: dict[str, Any]</code>  <code>property</code>","text":"<p>Variable definitions.</p>"},{"location":"api/#zappend.api.Config.zarr_version","title":"<code>zarr_version: int</code>  <code>property</code>","text":"<p>The configured Zarr version for the target dataset.</p>"},{"location":"api/#zappend.api.Config.__init__","title":"<code>__init__(config_dict)</code>","text":""},{"location":"api/#class-fileobj","title":"Class <code>FileObj</code>","text":"<p>An object that represents a file or directory in some filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The file or directory URI</p> required <code>storage_options</code> <code>dict[str, Any] | None</code> <p>Optional storage options specific to the protocol of the URI</p> <code>None</code> <code>fs</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem instance. Use with care, the filesystem must be consistent with uri and storage_options. For internal use only.</p> <code>None</code> <code>path</code> <code>str | None</code> <p>The path info the filesystem fs. Use with care, the path must be consistent with uri. For internal use only.</p> <code>None</code>"},{"location":"api/#zappend.api.FileObj.filename","title":"<code>filename: str</code>  <code>property</code>","text":"<p>The filename part of the URI.</p>"},{"location":"api/#zappend.api.FileObj.fs","title":"<code>fs: fsspec.AbstractFileSystem</code>  <code>property</code>","text":"<p>The filesystem.</p>"},{"location":"api/#zappend.api.FileObj.parent","title":"<code>parent: FileObj</code>  <code>property</code>","text":"<p>The parent file object.</p>"},{"location":"api/#zappend.api.FileObj.path","title":"<code>path: str</code>  <code>property</code>","text":"<p>The path of the file or directory into the filesystem.</p>"},{"location":"api/#zappend.api.FileObj.storage_options","title":"<code>storage_options: dict[str, Any] | None</code>  <code>property</code>","text":"<p>Storage options for creating the filesystem object.</p>"},{"location":"api/#zappend.api.FileObj.uri","title":"<code>uri: str</code>  <code>property</code>","text":"<p>The URI.</p>"},{"location":"api/#zappend.api.FileObj.__truediv__","title":"<code>__truediv__(rel_path)</code>","text":"<p>Overriden to call for_path(rel_path).</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>Relative path to append.</p> required"},{"location":"api/#zappend.api.FileObj.close","title":"<code>close()</code>","text":"<p>Close the filesystem used by this file object.</p>"},{"location":"api/#zappend.api.FileObj.delete","title":"<code>delete(recursive=False)</code>","text":"<p>Delete the file or directory represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>recursive</code> <code>bool</code> <p>Set to <code>True</code> to delete a non-empty directory.</p> <code>False</code>"},{"location":"api/#zappend.api.FileObj.exists","title":"<code>exists()</code>","text":"<p>Check if the file or directory represented by this file object exists.</p>"},{"location":"api/#zappend.api.FileObj.for_path","title":"<code>for_path(rel_path)</code>","text":"<p>Gets a new file object for the given relative path.</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>Relative path to append.</p> required <p>Returns:</p> Type Description <code>FileObj</code> <p>A new file object</p>"},{"location":"api/#zappend.api.FileObj.mkdir","title":"<code>mkdir()</code>","text":"<p>Create the directory represented by this file object.</p>"},{"location":"api/#zappend.api.FileObj.read","title":"<code>read(mode='rb')</code>","text":"<p>Read the contents of the file represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['rb'] | Literal['r']</code> <p>Read mode, must be \"rb\" or \"r\"</p> <code>'rb'</code> <p>Returns:</p> Type Description <code>bytes | str</code> <p>The contents of the file either as <code>bytes</code> if mode is \"rb\" or as <code>str</code></p> <code>bytes | str</code> <p>if mode is \"r\".</p>"},{"location":"api/#zappend.api.FileObj.write","title":"<code>write(data, mode=None)</code>","text":"<p>Write the contents of the file represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | bytes</code> <p>The data to write.</p> required <code>mode</code> <code>Literal['wb'] | Literal['w'] | Literal['ab'] | Literal['a'] | None</code> <p>Write mode, must be \"wb\", \"w\", \"ab\", or \"a\".</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes written.</p>"},{"location":"api/#types","title":"Types","text":""},{"location":"api/#zappend.api.SliceItem","title":"<code>zappend.api.SliceItem = str | FileObj | xr.Dataset | ContextManager[xr.Dataset] | SliceSource</code>  <code>module-attribute</code>","text":"<p>The possible types that can represent a slice dataset.</p>"},{"location":"api/#zappend.api.SliceCallable","title":"<code>zappend.api.SliceCallable = Type[SliceSource] | Callable[[...], SliceItem]</code>  <code>module-attribute</code>","text":"<p>This type is either a class derived from <code>SliceSource</code> or a function that  returns a <code>SliceItem</code>. Both can be invoked with any number of positional or  keyword arguments. The processing context, if used, must be named <code>ctx</code> and  must be either the 1st positional argument or a keyword argument. Its type  is <code>Context</code>.</p>"},{"location":"api/#zappend.api.ConfigItem","title":"<code>zappend.api.ConfigItem = FileObj | str | dict[str, Any]</code>  <code>module-attribute</code>","text":"<p>The possible types used to represent zappend configuration.</p>"},{"location":"api/#zappend.api.ConfigList","title":"<code>zappend.api.ConfigList = list[ConfigItem] | tuple[ConfigItem]</code>  <code>module-attribute</code>","text":"<p>A sequence of possible zappend configuration types.</p>"},{"location":"api/#zappend.api.ConfigLike","title":"<code>zappend.api.ConfigLike = ConfigItem | ConfigList | None</code>  <code>module-attribute</code>","text":"<p>Type for a zappend configuration-like object.</p>"},{"location":"cli/","title":"Command Line Interface Reference","text":"<p>After installation, the <code>zappend</code> command can be used from the terminal.  The following are the command's options and arguments:</p> <pre><code>Usage: zappend [OPTIONS] [SLICES]...\n\n  Create or update a Zarr datacube TARGET from slice datasets SLICES.\n\n  The zappend command concatenates the dataset SLICES along a given append\n  dimension, e.g., \"time\" (the default) for geospatial satellite observations.\n  Each append step is atomic, that is, the append operation is a transaction\n  that can be rolled back, in case the append operation fails. This ensures\n  integrity of the target data cube given by TARGET or in CONFIG.\n\nOptions:\n  -c, --config CONFIG    Configuration JSON or YAML file. If multiple are\n                         passed, subsequent configurations are incremental to\n                         the previous ones.\n  -t, --target TARGET    Target Zarr dataset path or URI. Overrides the\n                         'target_dir' configuration field.\n  --force-new            Force creation of a new target dataset. An existing\n                         target dataset (and its lock) will be permanently\n                         deleted before appending of slice datasets begins.\n                         WARNING: the deletion cannot be rolled back.\n  --dry-run              Run the tool without creating, changing, or deleting\n                         any files.\n  --traceback            Show Python traceback on error.\n  --version              Show version and exit.\n  --help-config json|md  Show configuration help and exit.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"config/","title":"Configuration Reference","text":"<p>Given here are all configuration settings for <code>zappend</code>.</p>"},{"location":"config/#target-outline","title":"Target Outline","text":""},{"location":"config/#append_dim","title":"<code>append_dim</code>","text":"<p>Type string. The name of the variadic append dimension. Defaults to <code>\"time\"</code>.</p>"},{"location":"config/#append_step","title":"<code>append_step</code>","text":"<p>If set, enforces a step size in the append dimension between two slices or just enforces a direction. Must be one of the following:</p> <ul> <li> <p>Arbitrary step size or not applicable.     Its value is <code>null</code>.</p> </li> <li> <p>Monotonically increasing.     Its value is <code>\"+\"</code>.</p> </li> <li> <p>Monotonically decreasing.     Its value is <code>\"-\"</code>.</p> </li> <li> <p>Type string.     A positive or negative time delta value, such as <code>12h</code>, <code>2D</code>, <code>-1D</code>.</p> </li> <li> <p>Type number.     A positive or negative numerical delta value.</p> </li> </ul> <p>Defaults to <code>null</code>.</p>"},{"location":"config/#fixed_dims","title":"<code>fixed_dims</code>","text":"<p>Type object. Specifies the fixed dimensions of the target dataset. Keys are dimension names, values are dimension sizes. The object's values are of type integer.</p>"},{"location":"config/#included_variables","title":"<code>included_variables</code>","text":"<p>Type array. Specifies the names of variables to be included in the target dataset. Defaults to all variables found in the first contributing dataset. The items of the list are of type string.</p>"},{"location":"config/#excluded_variables","title":"<code>excluded_variables</code>","text":"<p>Type array. Specifies the names of individual variables to be excluded  from all contributing datasets. The items of the list are of type string.</p>"},{"location":"config/#variables","title":"<code>variables</code>","text":"<p>Type object. Defines dimensions, encoding, and attributes for variables in the target dataset. Object property names refer to variable names. The special name <code>*</code> refers to all variables, which is useful for defining common values. The object's values are of type object. Variable metadata.</p> <ul> <li> <p><code>dims</code>:     The names of the variable's dimensions in the given order. Each dimension must exist in contributing datasets.     The items of the list are of type string.</p> </li> <li> <p><code>encoding</code>:     Variable Zarr storage encoding. Settings given here overwrite the encoding settings of the first contributing dataset.</p> <ul> <li><code>dtype</code>:     Must be one of <code>\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\", \"uint32\", \"int64\", \"uint64\", \"float32\", \"float64\"</code>.</li> <li><code>chunks</code>:     Must be one of the following:<ul> <li>Type array.     Chunk sizes for each dimension of the variable.     The items of the list must be one of the following:<ul> <li>Type integer.     Dimension is chunked using given size.</li> <li>Disable chunking in this dimension.     Its value is <code>null</code>.</li> </ul> </li> <li>Disable chunking in all dimensions.     Its value is <code>null</code>.</li> </ul> </li> <li><code>fill_value</code>:     Must be one of the following:<ul> <li>Type number.     A number of type and unit of the given storage <code>dtype</code>.</li> <li>Not-a-number. Can be used only if storage <code>dtype</code> is <code>float32</code> or <code>float64</code>.     Its value is <code>\"NaN\"</code>.</li> <li>No fill value.     Its value is <code>null</code>.</li> </ul> </li> <li><code>scale_factor</code>:     Scale factor for computing the in-memory value: <code>memory_value = scale_factor * storage_value + add_offset</code>.</li> <li><code>add_offset</code>:     Add offset for computing the in-memory value: <code>memory_value = scale_factor * storage_value + add_offset</code>.</li> <li><code>units</code>:     Units of the storage data type if memory data type is date/time.</li> <li><code>calendar</code>:     The calendar to be used if memory data type is date/time.</li> <li><code>compressor</code>:     Compressor definition. Set to <code>null</code> to disable data compression. Allowed parameters depend on the value of <code>id</code>.     The key <code>id</code> is required.<ul> <li><code>id</code>:     Compressor identifier</li> </ul> </li> <li><code>filters</code>:     List of filters. Set to <code>null</code> to not use filters.     The items of the list are of type object.     Filter definition. Allowed parameters depend on the value of <code>id</code>.     The key <code>id</code> is required.<ul> <li><code>id</code>:     Compressor identifier</li> </ul> </li> </ul> </li> <li> <p><code>attrs</code>:     Arbitrary variable metadata attributes.</p> </li> </ul>"},{"location":"config/#attrs","title":"<code>attrs</code>","text":"<p>Type object. Arbitrary dataset attributes. If <code>permit_eval</code> is set to <code>true</code>, string values may include Python expressions enclosed in <code>{{</code> and <code>}}</code> to dynamically compute attribute values; in the expression, the current dataset  is named <code>ds</code>. Refer to the user guide for more information.</p>"},{"location":"config/#attrs_update_mode","title":"<code>attrs_update_mode</code>","text":"<p>The mode used update target attributes from slice attributes. Independently of this setting, extra attributes configured by the <code>attrs</code> setting will finally be used to update the resulting target attributes. Must be one of the following:</p> <ul> <li> <p>Use attributes from first slice dataset and keep them.     Its value is <code>\"keep\"</code>.</p> </li> <li> <p>Replace existing attributes by attributes of last slice dataset.     Its value is <code>\"replace\"</code>.</p> </li> <li> <p>Update existing attributes by attributes of last slice dataset.     Its value is <code>\"update\"</code>.</p> </li> <li> <p>Ignore attributes from slice datasets.     Its value is <code>\"ignore\"</code>.</p> </li> </ul> <p>Defaults to <code>\"keep\"</code>.</p>"},{"location":"config/#zarr_version","title":"<code>zarr_version</code>","text":"<p>The Zarr version to be used. Its value is <code>2</code>. Defaults to <code>2</code>.</p>"},{"location":"config/#data-io-target","title":"Data I/O - Target","text":""},{"location":"config/#target_dir","title":"<code>target_dir</code>","text":"<p>Type string. The URI or local path of the target Zarr dataset. Must specify a directory whose parent directory must exist.</p>"},{"location":"config/#target_storage_options","title":"<code>target_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the URI of <code>target_dir</code>.</p>"},{"location":"config/#force_new","title":"<code>force_new</code>","text":"<p>Type boolean. Force creation of a new target dataset.  An existing target dataset (and its lock) will be permanently deleted before appending of slice datasets begins. WARNING: the deletion cannot be rolled back. Defaults to <code>false</code>.</p>"},{"location":"config/#data-io-slices","title":"Data I/O - Slices","text":""},{"location":"config/#slice_storage_options","title":"<code>slice_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the protocol of the URIs of contributing datasets.</p>"},{"location":"config/#slice_engine","title":"<code>slice_engine</code>","text":"<p>Type string. The name of the engine to be used for opening contributing datasets. Refer to the <code>engine</code> argument of the function <code>xarray.open_dataset()</code>.</p>"},{"location":"config/#slice_polling","title":"<code>slice_polling</code>","text":"<p>Defines how to poll for contributing datasets. Must be one of the following:</p> <ul> <li> <p>No polling, fail immediately if dataset is not available.     Its value is <code>false</code>.</p> </li> <li> <p>Poll using default values.     Its value is <code>true</code>.</p> </li> <li> <p>Type object.     Polling parameters.     The keys <code>interval</code>, <code>timeout</code> are required.</p> <ul> <li><code>interval</code>:     Polling interval in seconds.     Defaults to <code>2</code>.</li> <li><code>timeout</code>:     Polling timeout in seconds.     Defaults to <code>60</code>.</li> </ul> </li> </ul>"},{"location":"config/#slice_source","title":"<code>slice_source</code>","text":"<p>Type string. The fully qualified name of a class or function that receives a slice item as argument(s) and provides the slice dataset. If a class is given, it must be derived from <code>zappend.api.SliceSource</code>. If the function is a context manager, it must yield an <code>xarray.Dataset</code>. If a plain function is given, it must return any valid slice item type. Refer to the user guide for more information.</p>"},{"location":"config/#slice_source_kwargs","title":"<code>slice_source_kwargs</code>","text":"<p>Type object. Extra keyword-arguments passed to a configured <code>slice_source</code> together with each slice item.</p>"},{"location":"config/#persist_mem_slices","title":"<code>persist_mem_slices</code>","text":"<p>Type boolean. Persist in-memory slices and reopen from a temporary Zarr before appending them to the target dataset. This can prevent expensive re-computation of dask chunks at the cost of additional i/o. Defaults to <code>false</code>.</p>"},{"location":"config/#data-io-transactions","title":"Data I/O - Transactions","text":""},{"location":"config/#temp_dir","title":"<code>temp_dir</code>","text":"<p>Type string. The URI or local path of the directory that will be used to temporarily store rollback information.</p>"},{"location":"config/#temp_storage_options","title":"<code>temp_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the protocol of <code>temp_dir</code>.</p>"},{"location":"config/#disable_rollback","title":"<code>disable_rollback</code>","text":"<p>Type boolean. Disable rolling back dataset changes on failure. Effectively disables transactional dataset modifications, so use this setting with care. Defaults to <code>false</code>.</p>"},{"location":"config/#miscellaneous","title":"Miscellaneous","text":""},{"location":"config/#version","title":"<code>version</code>","text":"<p>Configuration schema version. Allows the schema to evolve while still preserving backwards compatibility. Its value is <code>1</code>. Defaults to <code>1</code>.</p>"},{"location":"config/#dry_run","title":"<code>dry_run</code>","text":"<p>Type boolean. If <code>true</code>, log only what would have been done, but don't apply any changes. Defaults to <code>false</code>.</p>"},{"location":"config/#permit_eval","title":"<code>permit_eval</code>","text":"<p>Type boolean. Allow for dynamically computed values in dataset attributes <code>attrs</code> using the syntax <code>{{ expression }}</code>.  Executing arbitrary Python expressions is a security risk, therefore this must be explicitly enabled. Refer to the user guide for more information. Defaults to <code>false</code>.</p>"},{"location":"config/#extra","title":"<code>extra</code>","text":"<p>Type object. Extra settings. Intended use is by a <code>slice_source</code> that expects an argument named <code>ctx</code> to access the extra settings and other configuration.</p>"},{"location":"config/#profiling","title":"<code>profiling</code>","text":"<p>Profiling configuration. Allows for runtime profiling of the processing. Must be one of the following:</p> <ul> <li> <p>Type boolean.     If set, profiling is enabled and output is logged using level <code>\"INFO\"</code>. Otherwise, profiling is disabled.</p> </li> <li> <p>Type string.     Profile path. Enables profiling and writes a profiling report to given path.</p> </li> <li> <p>Type object.     Detailed profiling configuration.</p> <ul> <li><code>enabled</code>:     Enable or disable profiling.</li> <li><code>path</code>:     Local file path for profiling output.</li> <li><code>log_level</code>:     Defaults to <code>\"INFO\"</code>.     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</li> <li><code>keys</code>:     Sort output according to the supplied column names. Refer to Stats.sort_stats(*keys).     Defaults to <code>[\"tottime\"]</code>.     Must be one of <code>\"calls\", \"cumulative\", \"cumtime\", \"file\", \"filename\", \"module\", \"ncalls\", \"pcalls\", \"line\", \"name\", \"nfl\", \"stdname\", \"time\", \"tottime\"</code>.</li> <li><code>restrictions</code>:     Used to limit the list down to the significant entries in the profiling report. Refer to Stats.print_stats(*restrictions).     The items of the list must be one of the following:<ul> <li>Type integer.     Select a count of lines.</li> <li>Type number.     Select a percentage of lines.</li> <li>Type string.     Pattern-match the standard name that is printed.</li> </ul> </li> </ul> </li> </ul>"},{"location":"config/#logging","title":"<code>logging</code>","text":"<p>Logging configuration. Must be one of the following:</p> <ul> <li> <p>Type boolean.     Shortform that enables logging to the console using log level <code>\"INFO\"</code>.</p> </li> <li> <p>Shortform that enables logging to the console using the specified log level.     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</p> </li> <li> <p>Type object.     Detailed logging configuration. For details refer to the dictionary schema of the Python module <code>logging.config</code>.     The key <code>version</code> is required.</p> <ul> <li><code>version</code>:     Its value is <code>1</code>.</li> <li><code>formatters</code>:     Formatter definitions. Each key is a formatter id and each value is an object describing how to configure the corresponding formatter.     The object's values are of type object.     Formatter configuration.<ul> <li><code>format</code>:     Format string in the given <code>style</code>.     Defaults to <code>\"%(message)s\"</code>.</li> <li><code>datefmt</code>:     Format string in the given <code>style</code> for the date/time portion.     Defaults to <code>\"%Y-%m-%d %H:%M:%S,uuu\"</code>.</li> <li><code>style</code>:     Must be one of <code>\"%\", \"{\", \"$\"</code>.</li> </ul> </li> <li><code>filters</code>:     Filter definitions. Each key is a filter id and each value is a dict describing how to configure the corresponding filter.     The object's values are of type object.     Filter configuration.</li> <li><code>handlers</code>:     Handler definitions. Each key is a handler id and each value is an object describing how to configure the corresponding handler.     The object's values are of type object.     Handler configuration. All keys other than the following are passed through as keyword arguments to the handler's constructor.     The key <code>class</code> is required.<ul> <li><code>class</code>:     The fully qualified name of the handler class. See logging handlers.</li> <li><code>level</code>:     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</li> <li><code>formatter</code>:     The id of the formatter for this handler.</li> <li><code>filters</code>:     A list of ids of the filters for this logger.     The items of the list are of type string.</li> </ul> </li> <li><code>loggers</code>:     Logger definitions. Each key is a logger name and each value is an object describing how to configure the corresponding logger. The tool's logger has the id <code>'zappend'</code>.     The object's values are of type object.     Logger configuration.<ul> <li><code>level</code>:     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</li> <li><code>propagate</code>:     The propagation setting of the logger.</li> <li><code>filters</code>:     A list of ids of the filters for this logger.     The items of the list are of type string.</li> <li><code>handlers</code>:     A list of ids of the handlers for this logger.     The items of the list are of type string.</li> </ul> </li> </ul> </li> </ul>"},{"location":"guide/","title":"User Guide","text":"<p>After installation, you can either use the <code>zappend</code> CLI command</p> <pre><code>zappend -t output/mycube.zarr inputs/*.nc\n</code></pre> <p>or the <code>zappend</code> Python function</p> <pre><code>from zappend.api import zappend\n\nzappend(os.listdir(\"inputs\"), target_dir=\"output/mycube.zarr\")\n</code></pre> <p>Both invocations will create the Zarr dataset <code>output/mycube.zarr</code> by concatenating the \"slice\" datasets provided in the <code>inputs</code> directory along their <code>time</code> dimension.  <code>target_dir</code> must specify a directory for the Zarr dataset. (Its parent directory must  exist.) Both the CLI command and the Python function can be run without any further  configuration provided the paths of the target dataset and the source slice datasets  are given. The target dataset path must point to a directory that will contain a Zarr  group to be created and updated. The slice dataset paths may be provided as Zarr as  well or in other data formats supported by the xarray.open_dataset()  function. Because we provided no additional configuration, the default append dimension <code>time</code> is used above.</p> <p>The target and slice datasets are allowed to live in filesystems other than the local one, if their paths are given as URIs prefixed with a filesystem protocol such as  <code>s3://</code> or <code>memory://</code>. Additional filesystem storage options may be specified via  dedicated configuration settings. More on this is given in section Data I/O below.</p> <p>Zarr Format v2</p> <p>By default, <code>zappend</code> uses the Zarr storage specification 2 and has only been tested with this version. The <code>zarr_version</code> setting can be used  to change it, e,g, to <code>3</code>, but any other value than <code>2</code> is currently unsupported.</p> <p>The tool takes care of generating the target dataset from slice datasets, but doesn't  care how the slice datasets are created. Hence, when using the Python <code>zappend()</code>  function, the slice datasets can be provided in various forms. More on this is given in section Slice Sources below.</p> <p>To run the <code>zappend</code> tool with configuration you can pass one or more configuration files using JSON or YAML format:</p> <pre><code>zappend -t output/mycube.zarr -c config.yaml inputs/*.nc\n</code></pre> <p>If multiple configuration files are passed, they will be merged into one by incrementally updating the first by subsequent ones. </p> <p>Environment Variables</p> <p>It is possible to include the values of environment variables in JSON or YAML  configuration files using the syntax <code>${ENV_VAR}</code> or just <code>$ENV_VAR</code>.</p> <p>You can pass configuration settings to the <code>zappend</code> Python function with  the optional <code>config</code> keyword argument. Other keyword arguments are  interpreted as individual configuration settings and will be merged into the  one given by <code>config</code> argument, if any. The <code>config</code> keyword argument can be  given as local file path or URL (type <code>str</code>) pointing to a JSON or YAML file. It can also be given as dictionary, or as a sequence of the aforementioned  types. Configuration sequences are again merged into one.</p> <pre><code>import os\nfrom zappend.api import zappend\n\nzappend(os.listdir(\"inputs\"), \n        config=[\"configs/base.yaml\",\n                \"configs/mycube.yaml\"], \n        target_dir=\"outputs/mycube.zarr\",\n        dry_run=True)\n</code></pre> <p>This remainder of this guide explains the how to use the various <code>zappend</code> configuration settings.</p> <p>Note</p> <p>We use the term Dataset in the same way <code>xarray</code> does: A dataset comprises any  number of multidimensional Data Variables, and usually 1-dimensional  Coordinate Variables that provide the labels for the dimensions used by the data  variables. A variable comprises the actual data array as well as metadata describing  the data dimensions, units, and encoding, such as chunking and compression.</p>"},{"location":"guide/#dataset-metadata","title":"Dataset Metadata","text":""},{"location":"guide/#outline","title":"Outline","text":"<p>If no further configuration is supplied, then the target dataset's outline and data  encoding is fully prescribed by the first slice dataset provided. By default, the dimension along which subsequent slice datasets are concatenated is <code>time</code>. If you use a different append dimension, the <code>append_dim</code> setting can be used to specify its name:</p> <pre><code>{\n    \"append_dim\": \"depth\"\n}\n</code></pre> <p>The configuration setting <code>append_step</code> can be used to validate the step sizes  between the labels of a coordinate variable associated with the append dimension.  Its value can be a number for numerical labels or a timedelta value of the form <code>&lt;count&gt;&lt;unit&gt;</code> for date/time labels. In the latter case <code>&lt;count&gt;</code> is an integer  and <code>&lt;units&gt;</code> is one of the possible numpy datetime units,  for example, <code>8h</code> (8 hours) or <code>2D</code> (two days). Numerical and timedelta values may be negative. <code>append_step</code> can also take the two special values <code>\"+\"</code> and  <code>\"-\"</code>. In this case it is just verified that the append labels are monotonically  increasing and decreasing, respectively.</p> <pre><code>{\n    \"append_dim\": \"time\",\n    \"append_step\": \"2D\"\n}\n</code></pre> <p>Other, non-variadic dimensions besides the append dimension can and should  be specified using the <code>fixed_dims</code> setting which is a mapping from dimension  name to the fixed dimension size, e.g.:</p> <pre><code>{\n    \"fixed_dims\": {\n        \"x\": 16384,\n        \"y\": 8192\n    }\n}\n</code></pre> <p>By default, without further configuration, all data variables seen in the first dataset slice will be included in the target dataset. If only a subset of  variables should be used from the slice dataset, they can be specified using the <code>included_variables</code> setting, which is a list of names of variables that will be included:</p> <pre><code>{\n    \"included_variables\": [\n        \"time\", \"y\", \"x\",\n        \"chl\", \n        \"tsm\"\n    ]\n}\n</code></pre> <p>Often, it is easier to specify which variables should be excluded:</p> <pre><code>{\n    \"excluded_variables\": [\"GridCellId\"]\n}\n</code></pre>"},{"location":"guide/#attributes","title":"Attributes","text":"<p>The target dataset should exploit information about itself using global  metadata attributes. There are three choices to update the global attributes of the target  dataset from slices. The configuration setting <code>attrs_update_mode</code>  controls how this is done:</p> <ul> <li><code>\"keep\"</code> - use attributes from first slice dataset and keep them (default);</li> <li><code>\"replace\"</code> - replace existing attributes by attributes of last slice dataset;</li> <li><code>\"update\"</code> - update existing attributes by attributes of last slice dataset;</li> <li><code>\"ignore\"</code> - ignore attributes from slice datasets.</li> </ul> <p>Extra attributes can be added using the optional configuration setting <code>attrs</code>:</p> <pre><code>{\n    \"attrs_update_mode\": \"keep\",\n    \"attrs\": {\n        \"Conventions\": \"CF-1.10\",\n        \"title\": \"SMOS Level 2C Soil Moisture 2-Days Composite\"\n    }\n}\n</code></pre> <p>Independently of the <code>attrs_update_mode</code> setting, extra attributes configured  by the <code>attrs</code> setting will always be used to update the resulting target  attributes. </p> <p>Attribute values in the <code>attrs</code> setting may also be computed dynamically using  the syntax <code>{{ expression }}</code>, where <code>expression</code> is an arbitrary Python  expression. For this to work, the setting <code>permit_eval</code> must be explicitly  set for security reasons:</p> <pre><code>{\n    \"permit_eval\": true,\n    \"attrs_update_mode\": \"keep\",\n    \"attrs\": {\n        \"time_coverage_start\": \"{{ ds.time[0] }}\", \n        \"time_coverage_end\": \"{{ ds.time[-1] }}\"\n    }\n}\n</code></pre> <p>Currently, the only variable accessible from expressions is <code>ds</code> which is  a reference to the current state of the target dataset after the last slice  append. It is of type  xarray.Dataset.</p> <p>Evil eval()</p> <p>The expressions in <code>{{ expression }}</code> are evaluated using the Python  eval() function. This can pose a threat to your application and environment.  Although <code>zappend</code> does not allow you to directly access Python built-in  functions via expressions, it should be used judiciously and with extreme  caution if used as part of a web service where configuration is injected  from the outside of your network.</p> <p>The following utility functions can be used as well and are handy if you need  to store the upper and lower bounds of coordinates as attribute values:</p> <ul> <li><code>lower_bound(array, ref: \"lower\"|\"upper\"|\"center\" = \"lower\")</code>:   Return the lower bound of a one-dimensional (coordinate) array <code>array</code>.</li> <li><code>upper_bound(array, ref: \"lower\"|\"upper\"|\"center\" = \"lower\")</code>:   Return the upper bound of a one-dimensional (coordinate) array <code>array</code>.</li> </ul> <p>The <code>ref</code> value specifies the reference within an array element that is used  as a basis for the boundary computation. E.g., if coordinate labels refer to  array element centers, pass <code>ref=\"center\"</code>.</p> <pre><code>{\n    \"attrs\": {\n        \"time_coverage_start\": \"{{ lower_bound(ds.time, 'center') }}\", \n        \"time_coverage_end\": \"{{ upper_bound(ds.time, 'center') }}\"\n    }\n}\n</code></pre>"},{"location":"guide/#variable-metadata","title":"Variable Metadata","text":"<p>Without any additional configuration, <code>zappend</code> uses the dimensions, attributes,  and encoding information from the data variables of the first slice dataset.  Encoding information is used only to the extent applicable to the Zarr format. Non-applicable encoding information will be reported by a warning log record  but is otherwise ignored. </p> <p>Variable metadata can be specified by the <code>variables</code> setting, which is a  mapping from variable name to a mapping that provides the dimensions, attributes,  and encoding information of data variables for the target dataset. All such  information is optional. The provided settings will be merged with the information retrieved from the data variables with same name included in the first dataset slice.</p> <p>A special \"variable name\" is the wildcard <code>*</code> that can be used to define default values for all variables:</p> <pre><code>{\n    \"variables\": {\n        \"*\": { \n        }\n    }\n}\n</code></pre> <p>If <code>*</code> is specified, the metadata for a particular variable is generated by merging the specific metadata for that variable into the common metadata given by <code>*</code>, which is eventually merged into metadata of the variable in the first dataset slice.</p> <p>Note</p> <p>Only metadata from the first slice dataset is used. Metadata of variables from subsequent slice datasets is ignored entirely.</p>"},{"location":"guide/#dimensions","title":"Dimensions","text":"<p>To ensure a slice variable has the expected dimensionality and shape, the <code>dims</code>  setting is used. The following example defines the dimensions of a data variable  named <code>chl</code> (Chlorophyll):</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"dims\": [\"time\", \"y\", \"x\"]\n        }\n    }\n}\n</code></pre> <p>An error will be raised if a variable from a subsequent slice has different dimensions.</p>"},{"location":"guide/#attributes_1","title":"Attributes","text":"<p>Extra variable attributes can be provided using the <code>attrs</code> setting:</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"attrs\": {\n                \"units\": \"mg/m^3\",\n                \"long_name\": \"chlorophyll_concentration\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"guide/#encoding","title":"Encoding","text":"<p>Encoding metadata specifies how array data is stored in the target dataset and includes  storage data type, packing, chunking, and compression. Encoding metadata for a given  variable is provided by the <code>encoding</code> setting. Since the encoding is often shared by  multiple variables the wildcard variable name <code>*</code> can often be of help.</p> <p>Verify encoding is as expected</p> <p>To verify that <code>zappend</code> uses the expected encoding for your variables create a  target dataset for testing from your first slice dataset and open it using  <code>ds = xarray.open_zarr(target_dir, decode_cf=False)</code>. Then inspect dataset <code>ds</code>  using the Python console or Jupyter Notebook (attribute <code>ds.&lt;var&gt;.encoding</code>). You can also inspect the Zarr directly by opening the <code>&lt;target_dir&gt;/&lt;var&gt;/.zarray</code> or <code>&lt;target_dir&gt;/.zmetadata</code> metadata JSON files.    </p>"},{"location":"guide/#chunking","title":"Chunking","text":"<p>Chunking refers to the subdivision of multidimensional data arrays into smaller multidimensional blocks. Using the Zarr format, such blocks become individual data files after optional data packing  and compression. The chunk sizes of the  dimensions of the multidimensional blocks therefore determine the number of  blocks used per data array and also their size. Hence, chunk sizes have  a very large impact on I/O performance of datasets, especially if they are persisted in remote filesystems such as S3. The chunk sizes are specified using the <code>chunks</code> setting in the encoding of each variable. The value of <code>chunks</code> can also be <code>null</code>, which means no chunking is  desired and the variable's data array will be persisted as one block. </p> <p>By default, the chunking of the coordinate variable corresponding to the append  dimension will be its dimension size in the first slice dataset. Often, the size  will be <code>1</code> or another small number. Since <code>xarray</code> loads coordinates eagerly  when opening a dataset, this can lead to performance issues if the target  dataset is served from object storage such as S3. The reason for this is that a  separate HTTP request is required for every single chunk. It is therefore very  advisable to set the chunks of that variable to a larger number using the  <code>chunks</code> setting. For other variables, you could still use a small chunk size  in the append dimension.</p> <p>Here is a typical chunking configuration for the append dimension <code>\"time\"</code>: </p> <pre><code>{\n    \"append_dim\": \"time\",\n    \"variables\": {\n        \"*\": {\n            \"encoding\": {\n                \"chunks\": null\n            }\n        },\n        \"time\": { \n            \"dims\": [\"time\"],\n            \"encoding\": {\n                \"chunks\": [1024]\n            }\n        },\n        \"chl\": { \n            \"dims\": [\"time\", \"y\", \"x\"],\n            \"encoding\": {\n                \"chunks\": [1, 2048, 2048]\n            }\n        }\n    }\n}\n</code></pre> <p>Sometimes, you may explicitly wish to not chunk a given dimension of a variable. If you know the size of that dimension in advance, you can then use its size as chunk size. But there are situations, where the final dimension size depends on some processing parameters. For example, you could define your own  slice source that takes a geodetic bounding box <code>bbox</code>  parameter to spatially crop your variables in the <code>x</code> and <code>y</code> dimensions.  If you want such dimensions to not be chunked, you can set their chunk sizes  to <code>null</code> (<code>None</code> in Python):</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"dims\": [\"time\", \"y\", \"x\"],\n            \"encoding\": {\n                \"chunks\": [1, null, null]\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"guide/#missing-data","title":"Missing Data","text":"<p>To indicate missing data in a variable data array, a dedicated no-data or missing value  can be specified by the <code>fill_value</code> setting. The value is given in a variable's storage  type and storage units, see next section Data Packing.</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"fill_value\": -999\n            }\n        }\n    }\n}\n</code></pre> <p>If the <code>fill_value</code> is not specified, the default is <code>NaN</code> (given as string <code>\"NaN\"</code>  in JSON) if the storage data type is floating point; it is <code>None</code> (<code>null</code> in JSON)  if the storage data types is integer, which effectively means, no fill value is used.  You can also explicitly set <code>fill_value</code> to <code>null</code> (<code>None</code> in Python) to not use one.</p> <p>Setting the <code>fill_value</code> for a variable can be important for saving storage space and  improving data I/O performance in many cases, because <code>zappend</code> does not write empty  array chunks - chunks that comprise missing data only, i.e.,  <code>slice.to_zarr(target_dir, write_empty_chunks=False, ...)</code>.</p>"},{"location":"guide/#data-packing","title":"Data Packing","text":"<p>Data packing refers to a simple lossy data compression method where 32- or 64-bit  floating point values are linearly scaled so that their value range can be fully or  partially represented by a lower precision integer data type. Packed values usually also give higher compression rates when using a <code>compressor</code>, see next section.</p> <p>Data packing is specified using the <code>scale_factor</code> and <code>add_offset</code> settings together with the storage data type setting <code>dtype</code>. The settings should be given as a triple:</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"dtype\": \"int16\",\n                \"scale_factor\": 0.005,\n                \"add_offset\": 0.0\n            }\n        }\n    }\n}\n</code></pre> <p>The in-memory value in its physical units for a given encoded value in storage is  computed according to </p> <pre><code>memory_value = scale_factor * storage_value + add_offset\n</code></pre> <p>Hence, the encoded value is computed from an in-memory value in physical units as</p> <pre><code>storage_value = (memory_value - add_offset) / scale_factor\n</code></pre> <p>You can compute <code>scale_factor</code> and <code>add_offset</code> from given data range in physical units according to</p> <pre><code>  add_offset = memory_value_min\n  scale_factor = (memory_value_max - memory_value_min) / (2 ** num_bits - 1)\n</code></pre> <p>with <code>num_bits</code> being the number of bits for the integer type to be used.</p>"},{"location":"guide/#compression","title":"Compression","text":"<p>Data compression is specified by the <code>compressor</code> setting, optionally paired with the <code>filters</code> setting: </p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"compressor\": {},\n                \"filters\": []\n            }\n        }\n    }\n}\n</code></pre> <p>By default, <code>zappend</code> uses default the default <code>blosc</code> compressor of Zarr, if not  specified. To explicitly disable compression you must set the <code>compressor</code> to <code>None</code>  (<code>null</code> in JSON).</p> <p>The usage of compressors and filters is best explained in dedicated sections of the  Zarr Tutorial, namely  Compressors and  Filters.</p>"},{"location":"guide/#data-io","title":"Data I/O","text":"<p>This section describes the configuration of how data is read and written.</p> <p>All input and output can be configured to take place in different filesystem. To  specify a filesystem other than the local one, you can use URIs and URLs for path  configuration settings such <code>target_dir</code> and <code>temp_dir</code> as well as for the slice  dataset paths. The filesystem is given by an URI's protocol prefix, such as <code>s3://</code>, which specifies the S3 filesystem. Additional storage parameters may be required to  access the data which can be provided by the settings <code>target_storage_options</code>,  <code>temp_storage_options</code>, and <code>slice_storage_options</code> which must be given as dictionaries  or JSON objects. The supported filesystems and their storage options are given by the  fsspec package.</p> <p>Tip</p> <p>You can use the <code>dry_run</code> setting to supress creation or modification of any files in the filesystem. This is useful for testing, e.g., make sure configuration is  valid and slice datasets can be read without errors. </p> <p>While the target dataset is being modified, a file lock is created used to effectively  prevent concurrent dataset modifications. After successfully appending a complete slice dataset, the lock is removed from the target. The lock file is written next to the target dataset, using the same filesystem and parent directory path. Its filename is the  filename of the target dataset suffixed by the extension <code>.lock</code>.</p>"},{"location":"guide/#transactions","title":"Transactions","text":"<p>Appending a slice dataset is an atomic operation to ensure the target dataset's  integrity. That is, in case a former append step failed, a rollback is performed to  restore the last valid state of the target dataset. The rollback takes place  immediately after a target dataset modification failed. The rollback include restoring  all changed files and removing added files. After the rollback you can analyse what went wrong and try to continue appending slices at the point it failed.</p> <p>To allow for rollbacks, a slice append operation is treated as a transaction, hence temporary files must be written, e.g., to record required rollback actions and to save backup files with the original data. The location of the temporary transaction  files can be configured using the optional <code>temp_dir</code> and <code>temp_storage_options</code> settings:</p> <p><pre><code>{\n    \"temp_dir\": \"memory://temp\"\n}\n</code></pre> The default value for <code>temp_dir</code> is your operating system's location for temporary  data (Python <code>tempfile.gettempdir()</code>). </p> <p>You can disable transaction management by specifying</p> <pre><code>{\n    \"disable_rollback\": true\n}\n</code></pre>"},{"location":"guide/#target-dataset","title":"Target Dataset","text":"<p>The <code>target_dir</code> setting is mandatory. If it is not specified in the configuration, it must be passed either as <code>--target</code> or <code>-t</code> option to the <code>zappend</code> command or as  <code>target_dir</code> keyword argument when using the <code>zappend</code> Python function. Note, the parent directory of <code>target_dir</code> must already exist.</p> <p>If the target path is given for another filesystem, additional storage options may be  passed using the optional <code>target_storage_options</code> setting. </p> <pre><code>{\n    \"target_dir\": \"s3://wqservices/cubes/chl-2023.zarr\",\n    \"target_storage_options\": {\n        \"anon\": false,\n        \"key\": \"...\",\n        \"secret\": \"...\",\n        \"endpoint_url\": \"https://s3.acme.org\"\n    }\n}\n</code></pre> <p>Sometimes you may want to start a new target dataset from scratch when calling  <code>zappend</code>. A typical case is testing if a given configuration yields the desired results. The configuration flag <code>force_new</code> can be used to delete existing target datasets (and an existing lock) upfront. </p> <pre><code>{\n    \"target_dir\": \"s3://wqservices/cubes/chl-2023.zarr\",\n    \"force_new\": true\n}\n</code></pre> <p>However, keep in mind that the deletion is not a transaction that can be rolled  back. Therefore, a log message with warning level will be emitted if the <code>force_new</code> flag is set.</p> <p>Setting <code>force_new</code></p> <p>The configuration flag <code>force_new</code> will force generating a new target dataset. If it already exists, it will be permanently deleted!  If the deletion fails, there will be no rollback.</p>"},{"location":"guide/#slice-datasets","title":"Slice Datasets","text":"<p>A slice dataset is the dataset that is appended for every slice item passed  to <code>zappend</code>. Slice datasets can be provided in various ways.</p> <ul> <li> <p>When using the zappend CLI command, slice items are passed as    command arguments where they point to slice datasets by local file paths or URIs.</p> </li> <li> <p>When using the zappend Python function, slice items are passed    using the <code>slices</code> argument, which is a Python iterable. You can pass a list or tuple   of slice items or provide a Python generator that provides the slice items.</p> </li> </ul> <p>Each slice item can be a local file path or URI of type <code>str</code> or <code>FileObj</code>,  a dataset of type <code>xarray.Dataset</code>, or a <code>SliceSource</code> object explained in more detail  below.</p>"},{"location":"guide/#paths-and-uris","title":"Paths and URIs","text":"<p>A slice item of type <code>str</code> is interpreted as local file path or URI, in the case  the path has a protocol prefix, such as <code>s3://</code> as described above.</p> <p>In the majority of <code>zappend</code> use cases the slice datasets to be appended to a target  dataset are passed as local file paths or URIs. A slice URI starts with a protocol  prefix, such as <code>s3://</code>, or <code>memory://</code>. Additional storage options may be required for the filesystem given by the URI's protocol. They may be specified using the <code>slice_storage_options</code> setting.</p> <pre><code>{\n    \"slice_storage_options\": {\n        \"anon\": true\n    }\n}\n</code></pre> <p>Sometimes, the slice dataset to be processed are not yet available, e.g.,  because another process is currently generating them. For such cases, the  <code>slice_polling</code> setting can be used. It provides the poll interval and the timeout  values in seconds. If this setting is used, and the slice dataset does not yet exist or  fails to open, the tool will retry to open it after the given interval. It will stop  doing so and exit with an error if the total time for opening the slice dataset exceeds the given timeout:</p> <pre><code>{\n    \"slice_polling\": {\n        \"interval\": 2,\n        \"timeout\": 600\n    } \n}\n</code></pre> <p>Or use default polling:</p> <pre><code>{\n    \"slice_polling\": true \n}\n</code></pre> <p>An alternative to providing the slice dataset as path or URI is using the  <code>zappend.api.FileObj</code> class, which combines a URI with dedicated filesystem  storage options.</p>"},{"location":"guide/#dataset-objects","title":"Dataset Objects","text":"<p>You can also use dataset objects of type  <code>xarray.Dataset</code>  as slice item. Such objects may originate from opening datasets from some storage, e.g.,  <code>xarray.open_dataset(slice_store, ...)</code> or by composing, aggregating, resampling  slice datasets from other datasets and dataset variables.</p> <p>Datasets are not closed automatically</p> <p>If you pass <code>xarray.Dataset</code> objects to <code>zappend</code> they will not be automatically closed. This may become be issue, if you have many datasets  and each one binds resources such as open file handles. Consider using a  slice source then, see below.</p> <p>Chunked data arrays of an <code>xarray.Dataset</code> are usually instances of  Dask arrays, to allow for out-of-core  computation of large datasets. As a dask array may represent complex and/or  expensive processing graphs, high CPU loads and memory consumption are common issues for computed slice datasets, especially if the specified target dataset chunking is  different from the slice dataset chunking. This may cause Dask graphs to be  computed multiple times if the source chunking overlaps multiple target chunks,  potentially causing large resource overheads while recomputing and/or reloading the same source chunks multiple times. In such cases it can help to \"terminate\" computations for  each slice by persisting the computed dataset first and then to reopen it. This can be  specified using the <code>persist_mem_slice</code> setting: </p> <pre><code>{\n    \"persist_mem_slice\": true\n}\n</code></pre> <p>If the flag is set, in-memory slices will be persisted to a temporary Zarr before  appending them to the target dataset. It may prevent expensive re-computation of chunks  at the cost of additional i/o. It therefore defaults to <code>false</code>.</p>"},{"location":"guide/#slice-sources","title":"Slice Sources","text":"<p>A slice source gives you full control about how a slice dataset is created, loaded,  or generated and how its bound resources, if any, are released. In its simplest form,  a slice source is a plain Python function that can take any arguments and returns  an <code>xarray.Dataset</code>:</p> <pre><code>import xarray as xr\n\n# Slice source argument `path` is just an example.\ndef get_dataset(path: str) -&gt; xr.Dataset:\n    # Provide dataset here. No matter how, e.g.:\n    return xr.open_dataset(path)\n</code></pre> <p>If you need cleanup code that is executed after the slice dataset has been appended,  you can turn your slice source function into a context manager (new in zappend v0.7):</p> <pre><code>from contextlib import contextmanager\nimport xarray as xr\n\n# Slice source argument `path` is just an example.\n@contextmanager\ndef get_dataset(path: str) -&gt; xr.Dataset:\n    # Bind any resources and provide dataset here, e.g.:\n    dataset = xr.open_dataset(path)\n    try:\n        # Yield (not return!) the dataset\n        yield dataset\n    finally:\n        # Cleanup code here, release any bound resources, e.g.:\n        dataset.close()\n</code></pre> <p>You can also implement your slice source as a class derived from the abstract  <code>zappend.api.SliceSource</code> class. Its interface methods are:</p> <ul> <li><code>get_dataset()</code>: a zero-argument method that returns the slice dataset of type    <code>xarray.Dataset</code>. You must implement this abstract method. </li> <li><code>close()</code>: Optional method. Put your cleanup code here.    (in zappend &lt; v0.7, the <code>close</code> method was called <code>dispose</code>).</li> </ul> <pre><code>import xarray as xr\nfrom zappend.api import SliceSource\n\nclass MySliceSource(SliceSource):\n    # Slice source argument `path` is just an example.\n    def __init__(self, path: str):\n        self.path = path\n        self.dataset = None\n\n    def get_dataset(self) -&gt; xr.Dataset:\n        # Bind any resources and provide dataset here, e.g.:\n        self.dataset = xr.open_dataset(self.path)\n        return self.dataset\n\n    def close(self):\n        # Cleanup code here, release any bound resources, e.g.:\n        if self.dataset is not None:\n            self.dataset.close()\n</code></pre> <p>You may prefer implementing a class because your slice source is complex and you want  to split its logic into separate methods. You may also just prefer classes as a matter  of your personal taste. Another advantage of using a class is that you can pass  instances of it as slice items to the <code>zappend</code> function without further configuration.  However, the intended use of a slice source is to configure it by specifying the  <code>slice_source</code> setting. In a JSON or YAML configuration file it specifies the fully  qualified name of the slice source function or class:</p> <pre><code>{\n    \"slice_source\": \"mymodule.MySliceSource\"\n}\n</code></pre> <p>If you use the <code>zappend</code> function, you can pass the function or class directly:</p> <pre><code>zappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"],\n        target_dir=\"target.zarr\",\n        slice_source=MySliceSource)\n</code></pre> <p>If the slice source setting is used, each slice item passed to <code>zappend</code> is passed as  argument(s) to your slice source.</p> <ul> <li>Slices passed to the <code>zappend</code> CLI command become slice source arguments    of type <code>str</code>.</li> <li> <p>Slice items passed to the <code>zappend</code> function via the <code>slices</code> argument can be of    any type, but the <code>tuple</code>, <code>list</code>, and <code>dict</code> types have a special meaning:</p> <ul> <li><code>tuple</code>: a pair of the form <code>(args, kwargs)</code>, where <code>args</code> is a list   or tuple of positional arguments and <code>kwargs</code> is a dictionary of keyword   arguments;</li> <li><code>list</code>: positional arguments only;</li> <li><code>dict</code>: keyword arguments only;</li> <li>Any other type is interpreted as single positional argument.</li> </ul> </li> </ul> <p>You can also pass extra keyword arguments to your slice source using the  <code>slice_source_kwargs</code> setting. Keyword arguments passed as slice items take  precedence, that is, they overwrite arguments passed by <code>slice_source_kwargs</code>.</p> <p>If your slice source has many parameters that stay the same for all slices you may  prefer providing parameters as configuration settings, rather than function or class arguments. This can be achieved using the <code>extra</code> setting:</p> <pre><code>{\n    \"extra\": {\n        \"quantiles\": [0.1, 0.5, 0.9],\n        \"use_threshold\": true,\n        \"filter\": \"gauss\"\n    }\n}\n</code></pre> <p>To access the settings in <code>extra</code> your slice source function or class constructor  must define a special argument named <code>ctx</code>. It must be a 1\u02e2\u1d57 positional argument or  a keyword argument. The argument <code>ctx</code> is the current processing context of type  <code>zappend.api.Context</code> that also contains the configuration. The settings in <code>extra</code> can be accessed using the dictionary returned from <code>ctx.config.extra</code>.</p> <p>Here is a more advanced example of a slice source that opens datasets from a given  file path and averages the values along the time dimension:</p> <pre><code>import numpy as np\nimport xarray as xr\nfrom zappend.api import Context\nfrom zappend.api import SliceSource\nfrom zappend.api import zappend\n\nclass MySliceSource(SliceSource):\n    def __init__(self, ctx: Context, slice_path: str):\n        self.quantiles = ctx.config.extra.get(\"quantiles\", [0.5])\n        self.slice_path = slice_path\n        self.ds = None\n\n    def get_dataset(self):\n        self.ds = xr.open_dataset(self.slice_path)\n        return self.get_agg_slice(self.ds)\n\n    def close(self):\n        if self.ds is not None:\n            self.ds.close()\n\n    def get_agg_slice(self, slice_ds: xr.Dataset) -&gt; xr.Dataset: \n        agg_slice_ds = slice_ds.quantile(self.quantiles, dim=\"time\")\n        # Re-introduce time dimension of size one\n        agg_slice_ds = agg_slice_ds.expand_dims(\"time\", axis=0)\n        agg_slice_ds.coords[\"time\"] = self.get_mean_time(slice_ds)\n        return agg_slice_ds \n\n    @classmethod\n    def get_mean_time(cls, slice_ds: xr.Dataset) -&gt; xr.DataArray:\n        time = slice_ds.time\n        t0 = time[0]\n        dt = time[-1] - t0\n        return xr.DataArray(np.array([t0 + dt / 2], \n                                     dtype=slice_ds.time.dtype), \n                            dims=\"time\")\n\nzappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"],\n        target_dir=\"target.zarr\",\n        slice_source=MySliceSource)\n</code></pre>"},{"location":"guide/#profiling","title":"Profiling","text":"<p>Runtime profiling is very important for understanding program runtime behavior  and performance. The configuration setting <code>profiling</code> can be used to analyse and improve the runtime performance of <code>zappend</code> itself as well as  the runtime performance of the computation of in-memory slices passed to the <code>zappend()</code> function.</p> <p>To log the output of the profiling with level <code>INFO</code> (see next section  Logging), you can use the value <code>true</code>:</p> <pre><code>{\n    \"profiling\": true\n}\n</code></pre> <p>If you like to see the output in a file too, then set <code>profiling</code> to the  desired local file path:</p> <pre><code>{\n    \"profiling\": \"perf.out\"\n}\n</code></pre> <p>You can also set <code>profiling</code> to an object that allows for fine-grained  control of the runtime logging: </p> <pre><code>{\n    \"profiling\": {\n        \"log_level\": \"WARNING\",\n        \"path\": \"perf.out\",\n        \"keys\": [\"tottime\", \"time\", \"ncalls\"]\n    }\n}\n</code></pre> <p>Please refer to section <code>profiling</code> in  the Configuration Reference for more details.</p>"},{"location":"guide/#logging","title":"Logging","text":"<p>The <code>zappend</code> logging output is configured using the <code>logging</code> setting. In the simplest case, if you just want logging output from <code>zappend</code> to the console:</p> <pre><code>{\n    \"logging\": true\n}\n</code></pre> <p>The above uses log level <code>INFO</code>. If you want a different log level,  just provide its name:</p> <pre><code>{\n    \"logging\": \"DEBUG\"\n}\n</code></pre> <p>If you also want logging output in a file or using a different format or if you want to see logging output of other Python modules, you can configure Python's logging  system following the logging dictionary schema. Given here is an example that logs <code>zappend</code>'s output to the console using  the <code>INFO</code> level (same as <code>\"logging\": true</code>):</p> <pre><code>{\n    \"logging\": {\n        \"version\": 1,\n        \"formatters\": {\n            \"normal\": {\n                \"format\": \"%(asctime)s %(levelname)s %(message)s\",\n                \"style\": \"%\"\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"normal\"\n            }\n        },\n        \"loggers\": {\n            \"zappend\": {\n                \"level\": \"INFO\",\n                \"handlers\": [\"console\"]\n            }\n        }\n    }\n}\n</code></pre> <p>Using the <code>loggers</code> entry you can configure the logger of other Python modules, e.g., <code>xarray</code> or <code>dask</code>. The logger used by the <code>zappend</code> tool is named <code>zappend</code>. </p>"},{"location":"requirements/","title":"Requirements","text":"<p>Given here are the original user requirements that have driven the development  of the <code>zappend</code> tool.</p>"},{"location":"requirements/#core-requirements","title":"Core Requirements","text":"<ul> <li>Create a target Zarr dataset by appending Zarr dataset slices along a    given append dimension, usually <code>time</code>.   </li> <li>The tool takes care of modifying the target dataset using the slices,   but doesn't care how the slice datasets are created.</li> <li>Slice datasets may be given as URIs with storage options or as    in-memory datasets of type    xarray.Dataset   or    xcube.core.mldataset.MultiLevelDataset.</li> <li>Target and slices are allowed to live in different filesystems.</li> <li>The tool is configurable. The configuration defines <ul> <li>the append dimension;</li> <li>optional target encoding for all or individual target variables;</li> <li>the target path into the target filesystem;</li> <li>optional target storage options;</li> <li>optional slice storage options.</li> </ul> </li> <li>The target chunking of the append dimension equals the size of the append    dimension in each slice and vice versa. </li> <li>The target encoding should allow for specifying the target storage chunking,    data type, and compression. </li> <li>The target encoding should also allow for packing floating point data into    integer data with fewer bits using scaling factor and offset.</li> <li>Detect coordinate variables and allow them to stay un-chunked.   This is important for coordinate variables containing or corresponding    to the append-dimension.</li> <li>If the target does not exist, it will be created from a copy of the first    slice. This first slice will specify any not-yet-configured properties   of the target dataset, e.g., the append dimension chunking.</li> <li>If the target exists, the slice will be appended. Check if the slice to be    appended is last. If not, refuse to append (alternative: insert but this is    probably difficult or error prone).</li> <li>Slices are appended in the order they are provided.</li> <li>If a slice is not yet available, wait and retry until it <ul> <li>exists, and</li> <li>is complete.</li> </ul> </li> <li>Check for each slice that it is valid. A valid slice<ul> <li>is self-consistent, </li> <li>has the same structure as target, and</li> <li>has an append dimension whose size is equal to the target chunking of   this dimension.</li> </ul> </li> <li>Before appending a slice, lock the target so that another tool invocation    can recognize it, e.g., write a lock file.</li> <li>If the target is locked, either wait until it becomes available or exit    with an error. The behaviour is controlled by a tool option.</li> <li>After successfully appending a slice, remove the lock from the target.</li> <li>Appending a slice shall be an atomic operation to ensure target dataset    integrity. That is, in case a former append step failed, a rollback must   be performed to restore the last valid state of the target. Rolling back   shall take place after an append failed, or before a new slice is appended,   or to sanitize a target to make it usable again. Rolling back shall    include restoring all changed files, removing all added files,    and removing any locks. </li> <li>The tool shall allow for continuing appending slices at the point   it failed.</li> <li>The tool shall offer a CLI and a Python API.<ul> <li>Using the CLI, slices are given as a variadic argument that provides the    file paths into the slice filesystem.</li> <li>Using the Python API, it shall be possible to provide the slices by    specifying a function that generates the slice datasets and an   iterable providing the arguments for the function.   This is similar how the Python <code>map()</code> built-in works.</li> </ul> </li> </ul>"},{"location":"requirements/#further-ideas","title":"Further Ideas","text":"<ul> <li>Allow for inserting and deleting slices.</li> <li>Allow for updating slices.   #29</li> <li>Allow specifying a constant delta between coordinates of the append dimension.   and verify coordinate deltas of append dimension to be constant.    #21</li> <li>Verify append dimension coordinates increase or decrease monotonically.    #20</li> <li>Integration with xcube:<ul> <li>Add xcube server API: Add endpoint to xcube server that works similar    to the CLI and also uses a similar request parameters.</li> <li>Use it in xcube data stores for the <code>write_data()</code> method, as a parameter    to enforce sequential writing of Zarr datasets as a robust option when a    plain write fails.</li> </ul> </li> </ul>"},{"location":"start/","title":"Getting Started","text":""},{"location":"start/#installation","title":"Installation","text":"<p><code>zappend</code> requires a Python v3.10+ environment. To install the latest released version from PyPI:</p> <pre><code>pip install zappend\n</code></pre> <p>To install the latest version for development, clone the repository, and with the repository\u2019s root directory as the current working directory execute:</p> <pre><code>pip install --editable .\n</code></pre>"},{"location":"start/#using-the-cli","title":"Using the CLI","text":"<p>Get usage help:</p> <pre><code>zappend --help\n</code></pre> <p>Get configuration help in Markdown format (json also available):</p> <pre><code>zappend --help-config md\n</code></pre> <p>Process list of local slice paths:</p> <pre><code>zappend --target target.zarr slice-1.nc slice-2.nc slice-3.nc\n</code></pre> <p>Process list of local slice paths with configuration in <code>config.yaml</code>:</p> <pre><code>zappend --config config.yaml slice-1.nc slice-2.nc slice-3.nc\n</code></pre>"},{"location":"start/#using-the-python-api","title":"Using the Python API","text":"<p>Process list of local slice paths:</p> <pre><code>from zappend.api import zappend\n\nzappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"], target_dir=\"target.zarr\")\n</code></pre> <p>Process list of slices stored in S3 configuration in <code>config</code>:</p> <pre><code>from zappend.api import zappend\n\nconfig = { \n    \"target_dir\": \"target.zarr\",\n    \"slice_storage_options\": {\n        \"key\": \"...\",               \n        \"secret\": \"...\",               \n    } \n}\n\nzappend((f\"s3:/mybucket/data/{name}\" \n         for name in [\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"]), \n        config=config)\n</code></pre> <p>Slice items can also be arguments passed to your custom slice source,  a function or class that provides the actual slice to be appended:</p> <pre><code>import xarray as xr\nfrom zappend.api import zappend\n\n\ndef get_dataset(path: str):\n    ds = xr.open_dataset(path)\n    return ds.drop_vars([\"ndvi_min\", \"ndvi_max\"])\n\nzappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"], \n        slice_source=get_dataset,\n        target_dir=\"target.zarr\")\n</code></pre> <p>For the details, please refer to the section Slice Sources in the  User Guide.</p>"}]}