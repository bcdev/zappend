{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#the-zappend-tool","title":"The zappend Tool","text":"<p><code>zappend</code> is a tool written in Python that is used for robustly creating and updating  Zarr datacubes from smaller dataset slices. It is built on top of the awesome Python packages xarray and zarr.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>The objective of <code>zappend</code> is to address recurring memory issues when generating large  geospatial datacubes using the Zarr format  by subsequently concatenating data slices along an append dimension, e.g., <code>time</code>  (the default) for geospatial satellite observations. Each append step is atomic, that is, the append operation is a transaction that can be  rolled back, in case the append operation fails. This ensures integrity of the target  data cube. </p>"},{"location":"#features","title":"Features","text":"<p>The <code>zappend</code> tool provides the following features:</p> <ul> <li>Locking: While the target dataset is being modified, a file lock is created,    effectively preventing concurrent dataset modifications.</li> <li>Transaction-based dataset appends: On failure during an append step,    the transaction is rolled back, so that the target dataset remains valid and    preserves its integrity.</li> <li>Filesystem transparency: The target dataset may be generated and updated in    any writable filesystems supported by the    fsspec package.    The same holds for the slice datasets to be appended.</li> <li>Dataset polling: The tool can be configured to wait for slice datasets to    become available. </li> <li>CLI and Python API: The tool can be used in a shell using the <code>zappend</code>   command or from Python. When used from Python using the    <code>zappend()</code> function, slice datasets can be passed as local file paths,    URIs, as datasets of type    xarray.Dataset, or as custom    zappend.api.SliceSource objects.</li> </ul>"},{"location":"#how-it-works","title":"How It Works","text":"<p>At its core, <code>zappend</code> calls the to_zarr() method of xarray.Dataset  for each dataset slice it receives and either creates the target dataset if it does  not exist yet or updates it with the current slice, if it already exists.</p> <p>If there is no target dataset yet, <code>zappend</code> does the following:</p> <ul> <li>get target dataset outline and encoding from configuration and first slice dataset;</li> <li>tailor first slice dataset according to target dataset outline;</li> <li>write target from first slice using target dataset encoding.</li> </ul> <p>If target dataset exists, then <code>zappend</code> will:</p> <ul> <li>get target dataset outline and encoding configuration and existing target dataset;</li> <li>for each subsequent slice dataset:<ul> <li>verify target and slice dataset are compatible;</li> <li>tailor slice dataset according to target dataset outline;</li> <li>remove all metadata including encoding and attributes from slice dataset;</li> <li>concatenate the \"naked\" slice dataset with the target dataset.</li> </ul> </li> </ul>"},{"location":"about/","title":"About zappend","text":""},{"location":"about/#changelog","title":"Changelog","text":"<p>You can find the complete <code>zappend</code> changelog  here. </p>"},{"location":"about/#reporting","title":"Reporting","text":"<p>If you have suggestions, ideas, feature requests, or if you have identified a malfunction or error, then please  post an issue. </p>"},{"location":"about/#contributions","title":"Contributions","text":"<p>The <code>zappend</code> project welcomes contributions of any form as long as you respect our  code of conduct and follow our  contribution guide.</p> <p>If you'd like to submit code or documentation changes, we ask you to provide a  pull request (PR)  here.  For code and configuration changes, your PR must be linked to a  corresponding issue. </p>"},{"location":"about/#development","title":"Development","text":"<p>Setup development environment:</p> <pre><code>pip install -r requirements.txt\npip install -r requirements-dev.txt\npip install -r requirements-docs.txt\n</code></pre>"},{"location":"about/#testing-and-coverage","title":"Testing and Coverage","text":"<p><code>zappend</code> uses pytest for unit-level testing  and code coverage analysis.</p> <pre><code>pytest --cov=zappend tests\n</code></pre>"},{"location":"about/#code-style","title":"Code Style","text":"<p><code>zappend</code> source code is formatted using the black tool.</p> <pre><code>black zappend\n</code></pre>"},{"location":"about/#documentation","title":"Documentation","text":"<p><code>zappend</code> documentation is built using the mkdocs tool.</p> <pre><code>pip install -r requirements-doc.txt\n\nmkdocs build\nmkdocs serve\nmkdocs gh-deploy\n</code></pre> <p>If the configuration JSON schema in <code>zappend/config/schema.py</code> changes then the configuration reference documentation <code>docs/config.md</code> must be  regenerated:</p> <pre><code>zappend --help-config md &gt; docs/config.md\n</code></pre>"},{"location":"about/#license","title":"License","text":"<p><code>zappend</code> is open source made available under the terms and conditions of the  MIT License.</p> <p>Copyright \u00a9 2024 Brockmann Consult Development</p>"},{"location":"api/","title":"Python API reference","text":"<p>All described objects can be imported from the <code>zappend.api</code> module.</p>"},{"location":"api/#function-zappend","title":"Function <code>zappend()</code>","text":""},{"location":"api/#zappend.api.zappend","title":"<code>zappend.api.zappend(slices, config=None, **kwargs)</code>","text":"<p>Robustly create or update a Zarr dataset from dataset slices.</p> <p>The <code>zappend</code> function concatenates the dataset slices from given <code>slices</code> along a given append dimension, e.g., <code>\"time\"</code> (the default) for geospatial satellite observations. Each append step is atomic, that is, the append operation is a transaction that can be rolled back, in case the append operation fails. This ensures integrity of the  target data cube <code>target_dir</code> given in <code>config</code> or <code>kwargs</code>.</p> <p>Parameters:</p> Name Type Description Default <code>slices</code> <code>Iterable[SliceObj | SliceFactory]</code> <p>An iterable that yields slice objects. A slice object is either a <code>str</code>, <code>xarray.Dataset</code>, <code>SliceSource</code> or a factory function that returns a slice object. If <code>str</code> is used, it is interpreted as local dataset path or dataset URI. If a URI is used, protocol-specific parameters apply, given by configuration parameter <code>slice_storage_options</code>.</p> required <code>config</code> <code>ConfigLike</code> <p>Processor configuration. May be a file path or URI, a <code>dict</code>, <code>None</code>, or a sequence of the aforementioned. If a sequence is used, subsequent configurations are incremental to the previous ones.</p> <code>None</code> <code>kwargs</code> <code>Any</code> <p>Additional configuration parameters. Can be used to pass or override configuration values in config.</p> <code>{}</code>"},{"location":"api/#function-to_slice_factories","title":"Function <code>to_slice_factories()</code>","text":""},{"location":"api/#zappend.api.to_slice_factories","title":"<code>zappend.api.to_slice_factories(slice_callable, slice_inputs)</code>","text":"<p>Utility function that generates slice factories for the given callable <code>slice_callable</code> and iterable of slice inputs <code>slice_inputs</code>.</p> <p>If the callable defines an argument named <code>ctx</code>, the current processing context of type Context will be passed to it. If it is defined as a positional argument, it must be the first one in <code>slice_callable</code>.</p> <p>The slice factories are returned as an iterator that generates a new slice factory (closure) using the to_slice_factory() for each item in <code>slice_inputs</code>. An item may be one of following:</p> <ul> <li><code>tuple</code>: a pair of the form <code>(args, kwargs)</code>, where <code>args</code> is a list   or tuple of positional arguments and <code>kwargs</code> is a dictionary of keyword   arguments;</li> <li><code>list</code>: positional arguments only;</li> <li><code>dict</code>: keyword arguments only;</li> <li>Any other type is interpreted as single positional argument.</li> </ul> <p>All items in <code>slice_inputs</code> should have the same type that matches the signature of <code>slice_callable</code>.</p> <p>Parameters:</p> Name Type Description Default <code>slice_callable</code> <code>Callable[[...], SliceObj] | Type[SliceSource]</code> <p>A callable that returns a slice object. Can also be the class of a custom SliceSource.</p> required <code>slice_inputs</code> <code>Iterable[Any]</code> <p>An iterable that yields the inputs passed to the given <code>slice_callable</code>.</p> required <p>Returns:     An iterator that returns a slice factory for each item in     <code>slice_args</code>.</p>"},{"location":"api/#function-to_slice_factory","title":"Function <code>to_slice_factory()</code>","text":""},{"location":"api/#zappend.api.to_slice_factory","title":"<code>zappend.api.to_slice_factory(slice_callable, *slice_args, **slice_kwargs)</code>","text":"<p>Utility function that generates a slice factory (closure) for the given callable and arguments.</p> <p>If the callable defines an argument named <code>ctx</code>, the current processing context of type Context will be passed to it. If <code>ctx</code> is defined as a positional argument, it must be the first one.</p> <p>Parameters:</p> Name Type Description Default <code>slice_callable</code> <code>Callable[[...], SliceObj] | Type[SliceSource]</code> <p>A callable that returns a slice object. Typically, the class of a custom SliceSource type will be passed.</p> required <code>slice_args</code> <code>Any</code> <p>The positional arguments that the slice factory will pass to <code>slice_callable</code>.</p> <code>()</code> <code>slice_kwargs</code> <code>Any</code> <p>The keyword arguments that the slice factory will pass to <code>slice_callable</code>.</p> <code>{}</code> <p>Returns:     A slice factory that receives the current processing context as     single argument.</p>"},{"location":"api/#class-slicesource","title":"Class <code>SliceSource</code>","text":"<p>             Bases: <code>ABC</code></p> <p>Represents a source for a slice dataset. Instances of this class are supposed to be used as context managers. The context manager provides the dataset instance by calling the get_dataset() method. When the context manager exits, the dispose() method is called.</p> <p>You may implement your own slice source class and define a slice factory function that creates instances of your slice source. Such functions can be passed input to the zappend() function, usually in the form of a closure to capture slice-specific information.</p> <p>The utility functions to_slice_factories() and to_slice_factory() ease passing slice-specific inputs to your custom slice source.</p> <p>Parameters:</p> Name Type Description Default <code>ctx</code> <code>Context</code> <p>The processing context.</p> required"},{"location":"api/#zappend.api.SliceSource.ctx","title":"<code>ctx: Context</code>  <code>property</code>","text":"<p>The processing context passed to the constructor.</p>"},{"location":"api/#zappend.api.SliceSource.dispose","title":"<code>dispose()</code>","text":"<p>Dispose this slice source. This should include cleaning up of any temporary resources.</p> <p>This method is not intended to be called directly. Instead, instances of this class are context managers and should be used with the Python <code>with</code> statement.</p> <p>This method is called exactly once for each instance of this class.</p>"},{"location":"api/#zappend.api.SliceSource.get_dataset","title":"<code>get_dataset()</code>  <code>abstractmethod</code>","text":"<p>Open this slice source, do some processing and return a dataset of type xarray.Dataset as result.</p> <p>This method is not intended to be called directly. Instead, instances of this class are context managers and should be used with the Python <code>with</code> statement.</p> <p>This method is called exactly once for each instance of this class.</p> <p>It should return a dataset that is compatible with target dataset:</p> <ul> <li>slice must have same fixed dimensions;</li> <li>append dimension must exist in slice.</li> </ul> <p>Returns:</p> Type Description <code>Dataset</code> <p>A slice dataset.</p>"},{"location":"api/#class-context","title":"Class <code>Context</code>","text":"<p>Provides access to configuration values and values derived from it.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Dict[str, Any]</code> <p>A validated configuration.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>target_dir</code> is missing in the configuration.</p>"},{"location":"api/#zappend.api.Context.append_dim_name","title":"<code>append_dim_name: str</code>  <code>property</code>","text":"<p>The name of the append dimension along which slice datasets will be concatenated. Defaults to <code>\"time\"</code>.</p>"},{"location":"api/#zappend.api.Context.append_step_size","title":"<code>append_step_size: int | float | str | None</code>  <code>property</code>","text":"<p>The enforced step size in the append dimension between two slices. Defaults to <code>None</code>.</p>"},{"location":"api/#zappend.api.Context.disable_rollback","title":"<code>disable_rollback: bool</code>  <code>property</code>","text":"<p>Whether to disable transaction rollbacks.</p>"},{"location":"api/#zappend.api.Context.dry_run","title":"<code>dry_run: bool</code>  <code>property</code>","text":"<p>Whether to run in dry mode.</p>"},{"location":"api/#zappend.api.Context.last_append_label","title":"<code>last_append_label: Any | None</code>  <code>property</code>","text":"<p>The last label found in the coordinate variable that corresponds to the append dimension. Its value is <code>None</code> if no such variable exists or the variable is empty or if append_step_size is <code>None</code>.</p>"},{"location":"api/#zappend.api.Context.persist_mem_slices","title":"<code>persist_mem_slices: bool</code>  <code>property</code>","text":"<p>Whether to persist in-memory slice datasets.</p>"},{"location":"api/#zappend.api.Context.slice_engine","title":"<code>slice_engine: str | None</code>  <code>property</code>","text":"<p>The configured slice engine to be used if a slice object is not a Zarr. If defined, it will be passed to the <code>xarray.open_dataset()</code> function.</p>"},{"location":"api/#zappend.api.Context.slice_polling","title":"<code>slice_polling: tuple[float, float] | tuple[None, None]</code>  <code>property</code>","text":"<p>The configured slice dataset polling. If slice polling is enabled, returns tuple (interval, timeout) in seconds, otherwise, return (None, None).</p>"},{"location":"api/#zappend.api.Context.slice_source","title":"<code>slice_source: Any | None</code>  <code>property</code>","text":"<p>The configured slice source, if any.</p>"},{"location":"api/#zappend.api.Context.slice_storage_options","title":"<code>slice_storage_options: dict[str, Any] | None</code>  <code>property</code>","text":"<p>The configured slice storage options to be used if a slice object is a Zarr.</p>"},{"location":"api/#zappend.api.Context.target_dir","title":"<code>target_dir: FileObj</code>  <code>property</code>","text":"<p>The configured directory that represents the target datacube in Zarr format.</p>"},{"location":"api/#zappend.api.Context.target_metadata","title":"<code>target_metadata: DatasetMetadata | None</code>  <code>property</code> <code>writable</code>","text":"<p>The metadata for the target dataset. May be <code>None</code> while the target dataset hasn't been created yet. Will be set, once the target dataset has been created from the first slice dataset.</p>"},{"location":"api/#zappend.api.Context.temp_dir","title":"<code>temp_dir: FileObj</code>  <code>property</code>","text":"<p>The configured directory used for temporary files such as rollback data.</p>"},{"location":"api/#zappend.api.Context.zarr_version","title":"<code>zarr_version: int</code>  <code>property</code>","text":"<p>The configured Zarr version for the target dataset.</p>"},{"location":"api/#zappend.api.Context.get_dataset_metadata","title":"<code>get_dataset_metadata(dataset)</code>","text":"<p>Get the dataset metadata from configuration and the given dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Dataset</code> <p>The dataset</p> required <p>Returns:</p> Type Description <code>DatasetMetadata</code> <p>The dataset metadata</p>"},{"location":"api/#class-fileobj","title":"Class <code>FileObj</code>","text":"<p>An object that represents a file or directory in some filesystem.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>The file or directory URI</p> required <code>storage_options</code> <code>dict[str, Any] | None</code> <p>Optional storage options specific to the protocol of the URI</p> <code>None</code> <code>fs</code> <code>AbstractFileSystem | None</code> <p>Optional fsspec filesystem instance. Use with care, the filesystem must be consistent with uri and storage_options. For internal use only.</p> <code>None</code> <code>path</code> <code>str | None</code> <p>The path info the filesystem fs. Use with care, the path must be consistent with uri. For internal use only.</p> <code>None</code>"},{"location":"api/#zappend.api.FileObj.filename","title":"<code>filename: str</code>  <code>property</code>","text":"<p>The filename part of the URI.</p>"},{"location":"api/#zappend.api.FileObj.fs","title":"<code>fs: fsspec.AbstractFileSystem</code>  <code>property</code>","text":"<p>The filesystem.</p>"},{"location":"api/#zappend.api.FileObj.parent","title":"<code>parent: FileObj</code>  <code>property</code>","text":"<p>The parent file object.</p>"},{"location":"api/#zappend.api.FileObj.path","title":"<code>path: str</code>  <code>property</code>","text":"<p>The path of the file or directory into the filesystem.</p>"},{"location":"api/#zappend.api.FileObj.storage_options","title":"<code>storage_options: dict[str, Any] | None</code>  <code>property</code>","text":"<p>Storage options for creating the filesystem object.</p>"},{"location":"api/#zappend.api.FileObj.uri","title":"<code>uri: str</code>  <code>property</code>","text":"<p>The URI.</p>"},{"location":"api/#zappend.api.FileObj.__truediv__","title":"<code>__truediv__(rel_path)</code>","text":"<p>Overriden to call for_path(rel_path).</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>Relative path to append.</p> required"},{"location":"api/#zappend.api.FileObj.close","title":"<code>close()</code>","text":"<p>Close the filesystem used by this file object.</p>"},{"location":"api/#zappend.api.FileObj.delete","title":"<code>delete(recursive=False)</code>","text":"<p>Delete the file or directory represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>recursive</code> <code>bool</code> <p>Set to <code>True</code> to delete a non-empty directory.</p> <code>False</code>"},{"location":"api/#zappend.api.FileObj.exists","title":"<code>exists()</code>","text":"<p>Check if the file or directory represented by this file object exists.</p>"},{"location":"api/#zappend.api.FileObj.for_path","title":"<code>for_path(rel_path)</code>","text":"<p>Gets a new file object for the given relative path.</p> <p>Parameters:</p> Name Type Description Default <code>rel_path</code> <code>str</code> <p>Relative path to append.</p> required <p>Returns:</p> Type Description <code>FileObj</code> <p>A new file object</p>"},{"location":"api/#zappend.api.FileObj.mkdir","title":"<code>mkdir()</code>","text":"<p>Create the directory represented by this file object.</p>"},{"location":"api/#zappend.api.FileObj.read","title":"<code>read(mode='rb')</code>","text":"<p>Read the contents of the file represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>mode</code> <code>Literal['rb'] | Literal['r']</code> <p>Read mode, must be \"rb\" or \"r\"</p> <code>'rb'</code> <p>Returns:</p> Type Description <code>bytes | str</code> <p>The contents of the file either as <code>bytes</code> if mode is \"rb\" or as <code>str</code></p> <code>bytes | str</code> <p>if mode is \"r\".</p>"},{"location":"api/#zappend.api.FileObj.write","title":"<code>write(data, mode=None)</code>","text":"<p>Write the contents of the file represented by this file object.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>str | bytes</code> <p>The data to write.</p> required <code>mode</code> <code>Literal['wb'] | Literal['w'] | Literal['ab'] | Literal['a'] | None</code> <p>Write mode, must be \"wb\", \"w\", \"ab\", or \"a\".</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of bytes written.</p>"},{"location":"api/#types","title":"Types","text":""},{"location":"api/#zappend.api.SliceObj","title":"<code>zappend.api.SliceObj = str | FileObj | xr.Dataset | SliceSource</code>  <code>module-attribute</code>","text":"<p>The possible types that can represent a slice dataset.</p>"},{"location":"api/#zappend.api.SliceFactory","title":"<code>zappend.api.SliceFactory = Callable[[Context], SliceObj]</code>  <code>module-attribute</code>","text":"<p>The type for a factory function that returns a slice object for a given processing context.</p>"},{"location":"api/#zappend.api.ConfigItem","title":"<code>zappend.api.ConfigItem = FileObj | str | dict[str, Any]</code>  <code>module-attribute</code>","text":"<p>The possible types used to represent zappend configuration.</p>"},{"location":"api/#zappend.api.ConfigList","title":"<code>zappend.api.ConfigList = list[ConfigItem] | tuple[ConfigItem]</code>  <code>module-attribute</code>","text":"<p>A sequence of possible zappend configuration types.</p>"},{"location":"api/#zappend.api.ConfigLike","title":"<code>zappend.api.ConfigLike = ConfigItem | ConfigList | None</code>  <code>module-attribute</code>","text":"<p>Type for a zappend configuration-like object.</p>"},{"location":"cli/","title":"Command Line Interface Reference","text":"<p>After installation, the <code>zappend</code> command can be used from the terminal.  The following are the command's options and arguments:</p> <pre><code>Usage: zappend [OPTIONS] [SLICES]...\n\n  Create or update a Zarr datacube TARGET from slice datasets SLICES.\n\n  The zappend command concatenates the dataset SLICES along a given append\n  dimension, e.g., `\"time\"` (the default) for geospatial satellite\n  observations. Each append step is atomic, that is, the append operation is a\n  transaction that can be rolled back, in case the append operation fails.\n  This ensures integrity of the target data cube given by TARGET or in CONFIG.\n\nOptions:\n  -c, --config CONFIG    Configuration JSON or YAML file. If multiple are\n                         passed, subsequent configurations are incremental to\n                         the previous ones.\n  -t, --target TARGET    Target Zarr dataset path or URI. Overrides the\n                         'target_dir' configuration field.\n  --dry-run              Run the tool without creating, changing, or deleting\n                         any files.\n  --version              Show version and exit.\n  --help-config json|md  Show configuration help and exit.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"config/","title":"Configuration Reference","text":""},{"location":"config/#version","title":"<code>version</code>","text":"<p>Configuration schema version. Allows the schema to evolve while still preserving backwards compatibility. Its value is <code>1</code>. Defaults to <code>1</code>.</p>"},{"location":"config/#zarr_version","title":"<code>zarr_version</code>","text":"<p>The Zarr version to be used. Its value is <code>2</code>. Defaults to <code>2</code>.</p>"},{"location":"config/#fixed_dims","title":"<code>fixed_dims</code>","text":"<p>Type object. Specifies the fixed dimensions of the target dataset. Keys are dimension names, values are dimension sizes. The object's values are of type integer.</p>"},{"location":"config/#append_dim","title":"<code>append_dim</code>","text":"<p>Type string. The name of the variadic append dimension. Defaults to <code>\"time\"</code>.</p>"},{"location":"config/#append_step","title":"<code>append_step</code>","text":"<p>If set, enforces a step size in the append dimension between two slices or just enforces a direction. Must be one of the following:</p> <ul> <li> <p>Arbitrary step size or not applicable.     Its value is <code>null</code>.</p> </li> <li> <p>Monotonically increasing.     Its value is <code>\"+\"</code>.</p> </li> <li> <p>Monotonically decreasing.     Its value is <code>\"-\"</code>.</p> </li> <li> <p>Type string.     A positive or negative time delta value, such as <code>12h</code>, <code>2D</code>, <code>-1D</code>.</p> </li> <li> <p>Type number.     A positive or negative numerical delta value.</p> </li> </ul> <p>Defaults to <code>null</code>.</p>"},{"location":"config/#included_variables","title":"<code>included_variables</code>","text":"<p>Type array. Specifies the names of variables to be included in the target dataset. Defaults to all variables found in the first contributing dataset. The items of the array are of type string.</p>"},{"location":"config/#excluded_variables","title":"<code>excluded_variables</code>","text":"<p>Type array. Specifies the names of individual variables to be excluded  from all contributing datasets. The items of the array are of type string.</p>"},{"location":"config/#variables","title":"<code>variables</code>","text":"<p>Type object. Defines dimensions, encoding, and attributes for variables in the target dataset. Object property names refer to variable names. The special name <code>*</code> refers to all variables, which is useful for defining common values. The object's values are of type object. Variable metadata.</p> <ul> <li> <p><code>dims</code>:     Type array.     The names of the variable's dimensions in the given order. Each dimension must exist in contributing datasets.     The items of the array are of type string.</p> </li> <li> <p><code>encoding</code>:     Type object.     Variable Zarr storage encoding. Settings given here overwrite the encoding settings of the first contributing dataset.</p> <ul> <li> <p><code>dtype</code>:     Storage data type     Must be one of <code>\"int8\", \"uint8\", \"int16\", \"uint16\", \"int32\", \"uint32\", \"int64\", \"uint64\", \"float32\", \"float64\"</code>.</p> </li> <li> <p><code>chunks</code>:     Storage chunking.     Must be one of the following:</p> <ul> <li> <p>Type array.     Chunk sizes in the order of the dimensions.     The items of the array are of type integer.</p> </li> <li> <p>Disable chunking.     Its value is <code>null</code>.</p> </li> </ul> </li> <li> <p><code>fill_value</code>:     Storage fill value.     Must be one of the following:</p> <ul> <li> <p>Type number.     A number of type and unit of the given storage <code>dtype</code>.</p> </li> <li> <p>Not-a-number. Can be used only if storage <code>dtype</code> is <code>float32</code> or <code>float64</code>.     Its value is <code>\"NaN\"</code>.</p> </li> <li> <p>No fill value.     Its value is <code>null</code>.</p> </li> </ul> </li> <li> <p><code>scale_factor</code>:     Type number.     Scale factor for computing the in-memory value: <code>memory_value = scale_factor * storage_value + add_offset</code>.</p> </li> <li> <p><code>add_offset</code>:     Type number.     Add offset for computing the in-memory value: <code>memory_value = scale_factor * storage_value + add_offset</code>.</p> </li> <li> <p><code>units</code>:     Type string.     Units of the storage data type if memory data type is date/time.</p> </li> <li> <p><code>calendar</code>:     Type string.     The calendar to be used if memory data type is date/time.</p> </li> <li> <p><code>compressor</code>:     Type array | null.     Compressor definition. Set to <code>null</code> to disable data compression.     The key <code>id</code> is required.</p> <ul> <li><code>id</code>:     Type string.</li> </ul> </li> <li> <p><code>filters</code>:     Type array | null.     Filters. Set to <code>null</code> to not use filters.     The items of the array are of type object.     Filter definition.     The key <code>id</code> is required.</p> <ul> <li><code>id</code>:     Type string.</li> </ul> </li> </ul> </li> <li> <p><code>attrs</code>:     Type object.     Arbitrary variable metadata attributes.</p> </li> </ul>"},{"location":"config/#target_dir","title":"<code>target_dir</code>","text":"<p>Type string. The URI or local path of the target Zarr dataset. Must specify a directory whose parent directory must exist.</p>"},{"location":"config/#target_storage_options","title":"<code>target_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the URI of <code>target_dir</code>.</p>"},{"location":"config/#slice_source","title":"<code>slice_source</code>","text":"<p>Type string. The fully qualified name of a class or function that provides a slice source for each slice item. If a class is given, it must be  derived from <code>zappend.api.SliceSource</code>. If a function is given, it must return an instance of  <code>zappend.api.SliceSource</code>. Refer to the user guide for more information.</p>"},{"location":"config/#slice_engine","title":"<code>slice_engine</code>","text":"<p>Type string. The name of the engine to be used for opening contributing datasets. Refer to the <code>engine</code> argument of the function <code>xarray.open_dataset()</code>.</p>"},{"location":"config/#slice_storage_options","title":"<code>slice_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the protocol of the URIs of contributing datasets.</p>"},{"location":"config/#slice_polling","title":"<code>slice_polling</code>","text":"<p>Defines how to poll for contributing datasets. Must be one of the following:</p> <ul> <li> <p>No polling, fail immediately if dataset is not available.     Its value is <code>false</code>.</p> </li> <li> <p>Poll using default values.     Its value is <code>true</code>.</p> </li> <li> <p>Type object.     Polling parameters.     The keys <code>interval</code>, <code>timeout</code> are required.</p> <ul> <li> <p><code>interval</code>:     Type number.     Polling interval in seconds.     Defaults to <code>2</code>.</p> </li> <li> <p><code>timeout</code>:     Type number.     Polling timeout in seconds.     Defaults to <code>60</code>.</p> </li> </ul> </li> </ul>"},{"location":"config/#persist_mem_slices","title":"<code>persist_mem_slices</code>","text":"<p>Type boolean. Persist in-memory slices and reopen from a temporary Zarr before appending them to the target dataset. This can prevent expensive re-computation of dask chunks at the cost of additional i/o. Defaults to <code>false</code>.</p>"},{"location":"config/#temp_dir","title":"<code>temp_dir</code>","text":"<p>Type string. The URI or local path of the directory that will be used to temporarily store rollback information.</p>"},{"location":"config/#temp_storage_options","title":"<code>temp_storage_options</code>","text":"<p>Type object. Options for the filesystem given by the protocol of <code>temp_dir</code>.</p>"},{"location":"config/#disable_rollback","title":"<code>disable_rollback</code>","text":"<p>Type boolean. Disable rolling back dataset changes on failure. Effectively disables transactional dataset modifications, so use this setting with care. Defaults to <code>false</code>.</p>"},{"location":"config/#profiling","title":"<code>profiling</code>","text":"<p>Profiling configuration. Allows for runtime profiling of the processing. Must be one of the following:</p> <ul> <li> <p>Type boolean.     If set, profiling is enabled and output is logged using level <code>\"INFO\"</code>. Otherwise, profiling is disabled.</p> </li> <li> <p>Type string.     Profile path. Enables profiling and writes a profiling report to given path.</p> </li> <li> <p>Type object.     Detailed profiling configuration.</p> <ul> <li> <p><code>enabled</code>:     Type boolean.     Enable or disable profiling.</p> </li> <li> <p><code>path</code>:     Type string.     Local file path for profiling output.</p> </li> <li> <p><code>log_level</code>:     Log level. Use <code>\"NOTSET\"</code> to disable logging.     Defaults to <code>\"INFO\"</code>.     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</p> </li> <li> <p><code>keys</code>:     Type array.     Sort output according to the supplied column names. Refer to Stats.sort_stats(*keys).     Defaults to <code>[\"tottime\"]</code>.     Must be one of <code>\"calls\", \"cumulative\", \"cumtime\", \"file\", \"filename\", \"module\", \"ncalls\", \"pcalls\", \"line\", \"name\", \"nfl\", \"stdname\", \"time\", \"tottime\"</code>.</p> </li> <li> <p><code>restrictions</code>:     Type array.     Used to limit the list down to the significant entries in the profiling report. Refer to Stats.print_stats(*restrictions).     The items of the array must be one of the following:</p> <ul> <li> <p>Type integer.     Select a count of lines.</p> </li> <li> <p>Type number.     Select a percentage of lines.</p> </li> <li> <p>Type string.     Pattern-match the standard name that is printed.</p> </li> </ul> </li> </ul> </li> </ul>"},{"location":"config/#logging","title":"<code>logging</code>","text":"<p>Type object. Logging configuration. For details refer to the dictionary schema of the Python module <code>logging.config</code>. The key <code>version</code> is required.</p> <ul> <li> <p><code>version</code>:     Logging schema version.     Its value is <code>1</code>.</p> </li> <li> <p><code>formatters</code>:     Type object.     Formatter definitions. Each key is a formatter id and each value is an object describing how to configure the corresponding formatter.     The object's values are of type object.     Formatter configuration.</p> <ul> <li> <p><code>format</code>:     Type string.     Format string in the given <code>style</code>.     Defaults to <code>\"%(message)s\"</code>.</p> </li> <li> <p><code>datefmt</code>:     Type string.     Format string in the given <code>style</code> for the date/time portion.     Defaults to <code>\"%Y-%m-%d %H:%M:%S,uuu\"</code>.</p> </li> <li> <p><code>style</code>:     Determines how the format string will be merged with its data.     Must be one of <code>\"%\", \"{\", \"$\"</code>.</p> </li> </ul> </li> <li> <p><code>filters</code>:     Type object.     Filter definitions. Each key is a filter id and each value is a dict describing how to configure the corresponding filter.     The object's values are of type object.     Filter configuration.</p> </li> <li> <p><code>handlers</code>:     Type object.     Handler definitions. Each key is a handler id and each value is an object describing how to configure the corresponding handler.     The object's values are of type object.     Handler configuration. All keys other than the following are passed through as keyword arguments to the handler's constructor.     The key <code>class</code> is required.</p> <ul> <li> <p><code>class</code>:     Type string.     The fully qualified name of the handler class. See logging handlers.</p> </li> <li> <p><code>level</code>:     The level of the handler.     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</p> </li> <li> <p><code>formatter</code>:     Type string.     The id of the formatter for this handler.</p> </li> <li> <p><code>filters</code>:     Type array.     A list of ids of the filters for this logger.     The items of the array are of type string.</p> </li> </ul> </li> <li> <p><code>loggers</code>:     Type object.     Logger definitions. Each key is a logger name and each value is an object describing how to configure the corresponding logger. The tool's logger has the id <code>'zappend'</code>.     The object's values are of type object.     Logger configuration.</p> <ul> <li> <p><code>level</code>:     The level of the logger.     Must be one of <code>\"CRITICAL\", \"ERROR\", \"WARNING\", \"INFO\", \"DEBUG\", \"NOTSET\"</code>.</p> </li> <li> <p><code>propagate</code>:     Type boolean.     The propagation setting of the logger.</p> </li> <li> <p><code>filters</code>:     Type array.     A list of ids of the filters for this logger.     The items of the array are of type string.</p> </li> <li> <p><code>handlers</code>:     Type array.     A list of ids of the handlers for this logger.     The items of the array are of type string.</p> </li> </ul> </li> </ul>"},{"location":"config/#dry_run","title":"<code>dry_run</code>","text":"<p>Type boolean. If <code>true</code>, log only what would have been done, but don't apply any changes. Defaults to <code>false</code>.</p>"},{"location":"guide/","title":"User Guide","text":"<p>After installation, you can either use the <code>zappend</code> CLI command</p> <pre><code>zappend -t output/mycube.zarr inputs/*.nc\n</code></pre> <p>or the <code>zappend</code> Python function</p> <pre><code>from zappend.api import zappend\n\nzappend(os.listdir(\"inputs\"), target_dir=\"output/mycube.zarr\")\n</code></pre> <p>Both invocations will create the Zarr dataset <code>output/mycube.zarr</code> by concatenating the \"slice\" datasets provided in the <code>inputs</code> directory along their <code>time</code> dimension.  <code>target_dir</code> must specify a directory for the Zarr dataset. (Its parent directory must  exist.) Both the CLI command and the Python function can be run without any further  configuration provided the paths of the target dataset and the source slice datasets  are given. The target dataset path must point to a directory that will contain a Zarr  group to be created and updated. The slice dataset paths may be provided as Zarr as  well or in other data formats supported by the xarray.open_dataset()  function. Because we provided no additional configuration, the default append dimension <code>time</code> is used above.</p> <p>The target and slice datasets are allowed to live in filesystems other than the local one, if their paths are given as URIs prefixed with a filesystem protocol such as  <code>s3://</code> or <code>memory://</code>. Additional filesystem storage options may be specified via  dedicated configuration settings. More on this is given in section Data I/O below.</p> <p>Zarr Format v2</p> <p>By default, <code>zappend</code> uses the Zarr storage specification 2 and has only been tested with this version. The <code>zarr_version</code> setting can be used  to change it, e,g, to <code>3</code>, but any other value than <code>2</code> is currently unsupported.</p> <p>The tool takes care of generating the target dataset from slice datasets, but doesn't  care how the slice datasets are created. Hence, when using the Python <code>zappend()</code>  function, the slice datasets can be provided in various forms. More on this is given in section Slice Sources below.</p> <p>To run the <code>zappend</code> tool with configuration you can pass one or more configuration files using JSON or YAML format:</p> <pre><code>zappend -t output/mycube.zarr -c config.yaml inputs/*.nc\n</code></pre> <p>If multiple configuration files are passed, they will be merged into one by incrementally updating the first by subsequent ones. </p> <p>Environment Variables</p> <p>It is possible to include the values of environment variables in JSON or YAML  configuration files using the syntax <code>${ENV_VAR}</code> or just <code>$ENV_VAR</code>.</p> <p>You can pass configuration settings to the <code>zappend</code> Python function with  the optional <code>config</code> keyword argument. Other keyword arguments are  interpreted as individual configuration settings and will be merged into the  one given by <code>config</code> argument, if any. The <code>config</code> keyword argument can be  given as local file path or URL (type <code>str</code>) pointing to a JSON or YAML file. It can also be given as dictionary, or as a sequence of the aforementioned  types. Configuration sequences are again merged into one.</p> <pre><code>import os\nfrom zappend.api import zappend\n\nzappend(os.listdir(\"inputs\"), \n        config=[\"configs/base.yaml\",\n                \"configs/mycube.yaml\"], \n        target_dir=\"outputs/mycube.zarr\",\n        dry_run=True)\n</code></pre> <p>This remainder of this guide explains the how to use the various <code>zappend</code> configuration settings.</p> <p>Note</p> <p>We use the term Dataset in the same way <code>xarray</code> does: A dataset comprises any  number of multidimensional Data Variables, and usually 1-dimensional  Coordinate Variables that provide the labels for the dimensions used by the data  variables. A variable comprises the actual data array as well as metadata describing  the data dimensions, units, and encoding, such as chunking and compression.</p>"},{"location":"guide/#dataset-outline","title":"Dataset Outline","text":"<p>If no further configuration is supplied, then the target dataset's outline and data  encoding is fully prescribed by the first slice dataset provided. By default, the dimension along which subsequent slice datasets are concatenated is <code>time</code>. If you use a different append dimension, the <code>append_dim</code> setting can be used to specify its name:</p> <pre><code>{\n    \"append_dim\": \"depth\"\n}\n</code></pre> <p>The configuration setting <code>append_step</code> can be used to validate the step sizes  between the labels of a coordinate variable associated with the append dimension.  Its value can be a number for numerical labels or a timedelta value of the form <code>&lt;count&gt;&lt;unit&gt;</code> for date/time labels. In the latter case <code>&lt;count&gt;</code> is an integer  and <code>&lt;units&gt;</code> is one of the possible numpy datetime units,  for example, <code>8h</code> (8 hours) or <code>2D</code> (two days). Numerical and timedelta values may be negative. <code>append_step</code> can also take the two special values <code>\"+\"</code> and  <code>\"-\"</code>. In this case it is just verified that the append labels are monotonically  increasing and decreasing, respectively.</p> <pre><code>{\n    \"append_dim\": \"time\",\n    \"append_step\": \"2D\"\n}\n</code></pre> <p>Other, non-variadic dimensions besides the append dimension can and should  be specified using the <code>fixed_dims</code> setting which is a mapping from dimension  name to the fixed dimension size, e.g.:</p> <pre><code>{\n    \"fixed_dims\": {\n        \"x\": 16384,\n        \"y\": 8192\n    }\n}\n</code></pre> <p>By default, without further configuration, all data variables seen in the first dataset slice will be included in the target dataset. If only a subset of  variables should be used from the slice dataset, they can be specified using the <code>included_variables</code> setting, which is a list of names of variables that will be included:</p> <pre><code>{\n    \"included_variables\": [\n        \"time\", \"y\", \"x\",\n        \"chl\", \n        \"tsm\"\n    ]\n}\n</code></pre> <p>Often, it is easier to specify which variables should be excluded:</p> <pre><code>{\n    \"excluded_variables\": [\"GridCellId\"]\n}\n</code></pre>"},{"location":"guide/#variable-metadata","title":"Variable Metadata","text":"<p>Without any additional configuration, <code>zappend</code> uses the dimensions, attributes,  and encoding information from the data variables of the first slice dataset.  Encoding information is used only to the extent applicable to the Zarr format. Non-applicable encoding information will be reported by a warning log record  but is otherwise ignored. </p> <p>Variable metadata can be specified by the <code>variables</code> setting, which is a  mapping from variable name to a mapping that provides the dimensions, attributes,  and encoding information of data variables for the target dataset. All such  information is optional. The provided settings will be merged with the information retrieved from the data variables with same name included in the first dataset slice.</p> <p>A special \"variable name\" is the wildcard <code>*</code> that can be used to define default values for all variables:</p> <pre><code>{\n    \"variables\": {\n        \"*\": { \n        }\n    }\n}\n</code></pre> <p>If <code>*</code> is specified, the metadata for a particular variable is generated by merging the specific metadata for that variable into the common metadata given by <code>*</code>, which is eventually merged into metadata of the variable in the first dataset slice.</p> <p>Note</p> <p>Only metadata from the first slice dataset is used. Metadata of variables from subsequent slice datasets is ignored entirely.</p>"},{"location":"guide/#dimensions","title":"Dimensions","text":"<p>To ensure a slice variable has the expected dimensionality and shape, the <code>dims</code>  setting is used. The following example defines the dimensions of a data variable  named <code>chl</code> (Chlorophyll):</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"dims\": [\"time\", \"y\", \"x\"]\n        }\n    }\n}\n</code></pre> <p>An error will be raised if a variable from a subsequent slice has different dimensions.</p>"},{"location":"guide/#attributes","title":"Attributes","text":"<p>Extra variable attributes can be provided using the <code>attrs</code> setting:</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"attrs\": {\n                \"units\": \"mg/m^3\",\n                \"long_name\": \"chlorophyll_concentration\"\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"guide/#encoding","title":"Encoding","text":"<p>Encoding metadata specifies how array data is stored in the target dataset and includes  storage data type, packing, chunking, and compression. Encoding metadata for a given  variable is provided by the <code>encoding</code> setting. Since the encoding is often shared by  multiple variables the wildcard variable name <code>*</code> can often be of help.</p> <p>Verify encoding is as expected</p> <p>To verify that <code>zappend</code> uses the expected encoding for your variables create a  target dataset for testing from your first slice dataset and open it using  <code>ds = xarray.open_zarr(target_dir, decode_cf=False)</code>. Then inspect dataset <code>ds</code>  using the Python console or Jupyter Notebook (attribute <code>ds.&lt;var&gt;.encoding</code>). You can also inspect the Zarr directly by opening the <code>&lt;target_dir&gt;/&lt;var&gt;/.zarray</code> or <code>&lt;target_dir&gt;/.zmetadata</code> metadata JSON files.    </p>"},{"location":"guide/#chunking","title":"Chunking","text":"<p>By default, the chunking of the coordinate variable corresponding to the append  dimension will be its dimension in the first slice dataset. Often, this will be one or  a small number. Since <code>xarray</code> loads coordinates eagerly when opening a dataset, this  can lead to performance issues if the target dataset is served from object storage such  as S3. This is because, a separate HTTP request is required for every single chunk. It  is therefore very advisable to set the chunks of that variable to a larger number using  the <code>chunks</code> setting. For other variables, the chunking within the append dimension may  stay small if desired:</p> <pre><code>{\n    \"variables\": {\n        \"time\": { \n            \"dims\": [\"time\"],\n            \"encoding\": {\n                \"chunks\": [1024]\n            }\n        },\n        \"chl\": { \n            \"dims\": [\"time\", \"y\", \"x\"],\n            \"encoding\": {\n                \"chunks\": [1, 2048, 2048]\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"guide/#missing-data","title":"Missing Data","text":"<p>To indicate missing data in a variable data array, a dedicated no-data or missing value  can be specified by the <code>fill_value</code> setting. The value is given in a variable's storage  type and storage units, see next section Data Packing.</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"fill_value\": -999\n            }\n        }\n    }\n}\n</code></pre> <p>If the <code>fill_value</code> is not specified, the default is <code>NaN</code> (given as string <code>\"NaN\"</code>  in JSON) if the storage data type is floating point; it is <code>None</code> (<code>null</code> in JSON)  if the storage data types is integer, which effectively means, no fill value is used.  You can also explicitly set <code>fill_value</code> to <code>null</code> (<code>None</code> in Python) to not use one.</p> <p>Setting the <code>fill_value</code> for a variable can be important for saving storage space and  improving data I/O performance in many cases, because <code>zappend</code> does not write empty  array chunks - chunks that comprise missing data only, i.e.,  <code>slice.to_zarr(target_dir, write_empty_chunks=False, ...)</code>.</p>"},{"location":"guide/#data-packing","title":"Data Packing","text":"<p>Data packing refers to a simple lossy data compression method where 32- or 64-bit  floating point values are linearly scaled so that their value range can be fully or  partially represented by a lower precision integer data type. Packed values usually also give higher compression rates when using a <code>compressor</code>, see next section.</p> <p>Data packing is specified using the <code>scale_factor</code> and <code>add_offset</code> settings together with the storage data type setting <code>dtype</code>. The settings should be given as a triple:</p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"dtype\": \"int16\",\n                \"scale_factor\": 0.005,\n                \"add_offset\": 0.0\n            }\n        }\n    }\n}\n</code></pre> <p>The in-memory value in its physical units for a given encoded value in storage is  computed according to </p> <pre><code>memory_value = scale_factor * storage_value + add_offset\n</code></pre> <p>Hence, the encoded value is computed from an in-memory value in physical units as</p> <pre><code>storage_value = (memory_value - add_offset) / scale_factor\n</code></pre> <p>You can compute <code>scale_factor</code> and <code>add_offset</code> from given data range in physical units according to</p> <pre><code>  add_offset = memory_value_min\n  scale_factor = (memory_value_max - memory_value_min) / (2 ** num_bits - 1)\n</code></pre> <p>with <code>num_bits</code> being the number of bits for the integer type to be used.</p>"},{"location":"guide/#compression","title":"Compression","text":"<p>Data compression is specified by the <code>compressor</code> setting, optionally paired with the <code>filters</code> setting: </p> <pre><code>{\n    \"variables\": {\n        \"chl\": { \n            \"encoding\": {\n                \"compressor\": {},\n                \"filters\": []\n            }\n        }\n    }\n}\n</code></pre> <p>By default, <code>zappend</code> uses default the default <code>blosc</code> compressor of Zarr, if not  specified. To explicitly disable compression you must set the <code>compressor</code> to <code>None</code>  (<code>null</code> in JSON).</p> <p>The usage of compressors and filters is best explained in dedicated sections of the  Zarr Tutorial, namely  Compressors and  Filters.</p>"},{"location":"guide/#data-io","title":"Data I/O","text":"<p>This section describes the configuration of how data is read and written.</p> <p>All input and output can be configured to take place in different filesystem. To  specify a filesystem other than the local one, you can use URIs and URLs for path  configuration settings such <code>target_dir</code> and <code>temp_dir</code> as well as for the slice  dataset paths. The filesystem is given by an URI's protocol prefix, such as <code>s3://</code>, which specifies the S3 filesystem. Additional storage parameters may be required to  access the data which can be provided by the settings <code>target_storage_options</code>,  <code>temp_storage_options</code>, and <code>slice_storage_options</code> which must be given as dictionaries  or JSON objects. The supported filesystems and their storage options are given by the  fsspec package.</p> <p>Tip</p> <p>You can use the <code>dry_run</code> setting to supress creation or modification of any files in the filesystem. This is useful for testing, e.g., make sure configuration is  valid and slice datasets can be read without errors. </p> <p>While the target dataset is being modified, a file lock is created used to effectively  prevent concurrent dataset modifications. After successfully appending a complete slice dataset, the lock is removed from the target. The lock file is written next to the target dataset, using the same filesystem and parent directory path. Its filename is the  filename of the target dataset suffixed by the extension <code>.lock</code>.</p>"},{"location":"guide/#transactions","title":"Transactions","text":"<p>Appending a slice dataset is an atomic operation to ensure the target dataset's  integrity. That is, in case a former append step failed, a rollback is performed to  restore the last valid state of the target dataset. The rollback takes place  immediately after a target dataset modification failed. The rollback include restoring  all changed files and removing added files. After the rollback you can analyse what went wrong and try to continue appending slices at the point it failed.</p> <p>To allow for rollbacks, a slice append operation is treated as a transaction, hence temporary files must be written, e.g., to record required rollback actions and to save backup files with the original data. The location of the temporary transaction  files can be configured using the optional <code>temp_dir</code> and <code>temp_storage_options</code> settings:</p> <p><pre><code>{\n    \"temp_dir\": \"memory://temp\"\n}\n</code></pre> The default value for <code>temp_dir</code> is your operating system's location for temporary  data (Python <code>tempfile.gettempdir()</code>). </p> <p>You can disable transaction management by specifying</p> <pre><code>{\n    \"disable_rollback\": true\n}\n</code></pre>"},{"location":"guide/#target-dataset","title":"Target Dataset","text":"<p>The <code>target_dir</code> setting is mandatory. If it is not specified in the configuration, it must be passed either as <code>--target</code> or <code>-t</code> option to the <code>zappend</code> command or as  <code>target_dir</code> keyword argument when using the <code>zappend</code> Python function. Note, the parent directory of <code>target_dir</code> must already exist.</p> <p>If the target path is given for another filesystem, additional storage options may be  passed using the optional <code>target_storage_options</code> setting. </p> <pre><code>{\n    \"target_dir\": \"s3://wqservices/cubes/chl-2023.zarr\",\n    \"target_storage_options\": {\n        \"anon\": false,\n        \"key\": \"...\",\n        \"secret\": \"...\",\n        \"endpoint_url\": \"https://s3.acme.org\"\n    }\n}\n</code></pre>"},{"location":"guide/#slice-datasets","title":"Slice Datasets","text":"<p>If the slice paths passed to the <code>zappend</code> tool are given as URIs, additional storage options may be provided for the filesystem given by the  URI's protocol. They may be specified using the <code>slice_storage_options</code> setting.</p> <p>Sometimes, the slice dataset to be processed are not yet available, e.g.,  because another process is currently generating them. For such cases, the  <code>slice_polling</code> setting can be used. It provides the poll interval and the timeout  values in seconds. If this setting is used, and the slice dataset does not yet exist or  fails to open, the tool will retry to open it after the given interval. It will stop  doing so and exit with an error if the total time for opening the slice dataset exceeds the given timeout:</p> <pre><code>{\n    \"slice_polling\": {\n        \"interval\": 2,\n        \"timeout\": 600\n    } \n}\n</code></pre> <p>Or use default polling:</p> <pre><code>{\n    \"slice_polling\": true \n}\n</code></pre>"},{"location":"guide/#slice-sources","title":"Slice Sources","text":"<p>A slice source is an object that provides a slice dataset of type <code>xarray.Dataset</code>  for given parameters of any type.</p> <p>The optional <code>slice_source</code> configuration setting is used to specify a custom  slice source. If not specified, <code>zappend</code> selects the slice source based on the type  of a given slice object. These types are described in following subsections.</p> <p>If given, the value of the <code>slice_source</code> setting is a class derived from <code>zappend.api.SliceSource</code>, or a function that creates an instance of  <code>zappend.api.SliceSource</code>, or the fully qualified name of the aforementioned.  In the case <code>slice_source</code> is given, the slices argument passed to the CLI  command and Python function become parameters to the specified class constructor  or factory function. The individual slice items in the <code>SLICES</code> arguments of the <code>zappend</code> CLI  command are of type <code>str</code>, typically interpreted as file paths or URIs. The individual slice items passed in the <code>slices</code> argument of the <code>zappend.api.zappend()</code> function can be of any type, but the <code>tuple</code>, <code>list</code>,  and <code>dict</code> types have a special meaning:</p> <ul> <li><code>tuple</code>: a pair of the form <code>(args, kwargs)</code>, where <code>args</code> is a list   or tuple of positional arguments and <code>kwargs</code> is a dictionary of keyword   arguments;</li> <li><code>list</code>: positional arguments only;</li> <li><code>dict</code>: keyword arguments only;</li> <li>Any other type is interpreted as single positional argument.</li> </ul> <p>In addition, your class constructor or factory function specified by <code>slice_source</code>  may specify a positional or keyword argument named <code>ctx</code>, which will receive the  current processing context of type <code>zappend.api.Context</code>. </p> <p>If the <code>slice_source</code> setting is not specified, the slice items passed as <code>slices</code> argument to the <code>zappend</code> Python function can be one of the types described in the following subsections.</p>"},{"location":"guide/#str-and-zappendapifileobj","title":"<code>str</code> and <code>zappend.api.FileObj</code>","text":"<p>A slice object of type <code>str</code> is interpreted as local file path or URI, in the case  the path has a protocol prefix, such as <code>s3://</code>.</p> <p>An alternative to providing the slice dataset as path or URI is using the <code>FileObj</code>  class, which combines a URI with dedicated filesystem storage options.</p> <pre><code>from zappend.api import FileObj\n\nslice_obj = FileObj(slice_uri, storage_options=dict(...)) \n</code></pre>"},{"location":"guide/#xarraydataset","title":"<code>xarray.Dataset</code>","text":"<p>In-memory slice objects can be passed as xarray.Dataset objects.  Such objects may originate from opening datasets from some storage </p> <pre><code>import xarray as xr\n\nslice_obj = xr.open_dataset(slice_store, ...) \n</code></pre> <p>or by composing, aggregating, resampling slice datasets from other datasets and  data variables. To allow for out-of-core computation of large datasets Dask arrays are used by both <code>xarray</code> and <code>zarr</code>. As a dask array may represent complex and/or  expensive processing graphs, high CPU loads and memory consumption are common issues for computed slice datasets, especially if the specified target dataset chunking is  different from the slice dataset chunking. This may cause Dask graphs to be  computed multiple times if the source chunking overlaps multiple target chunks,  potentially causing large resource overheads while recomputing and/or reloading the same source chunks multiple times.</p> <p>In such cases it can help to \"terminate\" such computations for each slice by  persisting the computed dataset first and then to reopen it. This can be specified  using the <code>persist_mem_slice</code> setting: </p> <pre><code>{\n    \"persist_mem_slice\": true\n}\n</code></pre> <p>If the flag is set, in-memory slices will be persisted to a temporary Zarr before  appending them to the target dataset. It may prevent expensive re-computation of chunks  at the cost of additional i/o. It therefore defaults to <code>false</code>.</p>"},{"location":"guide/#zappendapislicesource","title":"<code>zappend.api.SliceSource</code>","text":"<p>Often you want to perform some custom cleanup after a slice has been processed and appended to the target dataset. In this case you can write your own  <code>zappend.api.SliceSource</code> by implementing its <code>get_dataset()</code> and <code>dispose()</code> methods. </p> <p>Slice source instances are supposed to be created by slice factories, see  subsection below.</p>"},{"location":"guide/#zappendapislicefactory","title":"<code>zappend.api.SliceFactory</code>","text":"<p>A slice factory is a 1-argument function that receives a processing context of type <code>zappend.api.Context</code> and yields a slice dataset object of one of the types described above. Since a slice factory cannot have additional arguments, it is  normally defined as a closure  to capture slice-specific information.</p> <p>In the following example, the actual slice dataset is computed from averaging another  dataset. A <code>SliceSource</code> is used to close the datasets after the slice has been  processed. Slice factories are created from the custom slice source and the slice paths using the utility function to_slice_factories():</p> <pre><code>import numpy as np\nimport xarray as xr\nfrom zappend.api import SliceSource\nfrom zappend.api import to_slice_factories\nfrom zappend.api import zappend\n\ndef get_mean_time(slice_ds: xr.Dataset) -&gt; xr.DataArray:\n    time = slice_ds.time\n    t0 = time[0]\n    dt = time[-1] - t0\n    return xr.DataArray(np.array([t0 + dt / 2], \n                                 dtype=slice_ds.time.dtype), \n                        dims=\"time\")\n\ndef get_mean_slice(slice_ds: xr.Dataset) -&gt; xr.Dataset: \n    mean_slice_ds = slice_ds.mean(\"time\")\n    mean_slice_ds = mean_slice_ds.expand_dims(\"time\", axis=0)\n    mean_slice_ds.coords[\"time\"] = get_mean_time(slice_ds)\n    return mean_slice_ds \n\nclass MySliceSource(SliceSource):\n    def __init__(self, ctx, slice_path):\n        super().__init__(ctx)\n        self.slice_path = slice_path\n        self.ds = None\n        self.mean_ds = None\n\n    def get_dataset(self):\n        self.ds = xr.open_dataset(self.slice_path)\n        self.mean_ds = get_mean_slice(self.ds)\n        return self.mean_ds\n\n    def dispose(self):\n        if self.ds is not None:\n            self.ds.close()\n            self.ds = None\n        if self.mean_ds is not None:\n            self.mean_ds.close()\n            self.mean_ds = None\n\nzappend(to_slice_factories(MySliceSource, [\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"]),\n        target_dir=\"target.zarr\")\n</code></pre> <p>Note, the above example can be simplified by using the <code>slice_source</code> setting directly:</p> <pre><code>zappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"],\n        target_dir=\"target.zarr\",\n        slice_source=MySliceSource)\n</code></pre>"},{"location":"guide/#profiling","title":"Profiling","text":"<p>Runtime profiling is very important for understanding program runtime behavior  and performance. The configuration setting <code>profiling</code> can be used to analyse and improve the runtime performance of <code>zappend</code> itself as well as  the runtime performance of the computation of in-memory slices passed to the <code>zappend()</code> function.</p> <p>To log the output of the profiling with level <code>INFO</code> (see next section  Logging), you can use the value <code>true</code>:</p> <pre><code>{\n    \"profiling\": true\n}\n</code></pre> <p>If you like to see the output in a file too, then set <code>profiling</code> to the  desired local file path:</p> <pre><code>{\n    \"profiling\": \"perf.out\"\n}\n</code></pre> <p>You can also set <code>profiling</code> to an object that allows for fine-grained  control of the runtime logging: </p> <pre><code>{\n    \"profiling\": {\n        \"log_level\": \"WARNING\",\n        \"path\": \"perf.out\",\n        \"keys\": [\"tottime\", \"time\", \"ncalls\"]\n    }\n}\n</code></pre> <p>Please refer to section <code>profiling</code> in  the Configuration Reference for more details.</p>"},{"location":"guide/#logging","title":"Logging","text":"<p>The <code>zappend</code> logging output is configured using the <code>logging</code> setting. Its configuration follows exactly the  dictionary schema of the Python module <code>logging.config</code>. The logger used by the <code>zappend</code> tool is named  <code>zappend</code>. Note that you can also configure the logger of other Python modules, e.g., <code>xarray</code> or <code>dask</code> using an entry in the <code>loggers</code> setting.</p> <p>Given here is an example that logs <code>zappend</code>'s output to the console using  the INFO level:</p> <pre><code>{\n    \"logging\": {\n        \"version\": 1,\n        \"formatters\": {\n            \"normal\": {\n                \"format\": \"%(asctime)s %(levelname)s %(message)s\",\n                \"style\": \"%\"\n            }\n        },\n        \"handlers\": {\n            \"console\": {\n                \"class\": \"logging.StreamHandler\",\n                \"formatter\": \"normal\"\n            }\n        },\n        \"loggers\": {\n            \"zappend\": {\n                \"level\": \"INFO\",\n                \"handlers\": [\"console\"]\n            }\n        }\n    }\n}\n</code></pre>"},{"location":"requirements/","title":"Requirements","text":"<p>Given here are the original user requirements that have driven the development  of the <code>zappend</code> tool.</p>"},{"location":"requirements/#core-requirements","title":"Core Requirements","text":"<ul> <li>Create a target Zarr dataset by appending Zarr dataset slices along a    given append dimension, usually <code>time</code>.   </li> <li>The tool takes care of modifying the target dataset using the slices,   but doesn't care how the slice datasets are created.</li> <li>Slice datasets may be given as URIs with storage options or as    in-memory datasets of type    xarray.Dataset   or    xcube.core.mldataset.MultiLevelDataset.</li> <li>Target and slices are allowed to live in different filesystems.</li> <li>The tool is configurable. The configuration defines <ul> <li>the append dimension;</li> <li>optional target encoding for all or individual target variables;</li> <li>the target path into the target filesystem;</li> <li>optional target storage options;</li> <li>optional slice storage options.</li> </ul> </li> <li>The target chunking of the append dimension equals the size of the append    dimension in each slice and vice versa. </li> <li>The target encoding should allow for specifying the target storage chunking,    data type, and compression. </li> <li>The target encoding should also allow for packing floating point data into    integer data with fewer bits using scaling factor and offset.</li> <li>Detect coordinate variables and allow them to stay un-chunked.   This is important for coordinate variables containing or corresponding    to the append-dimension.</li> <li>If the target does not exist, it will be created from a copy of the first    slice. This first slice will specify any not-yet-configured properties   of the target dataset, e.g., the append dimension chunking.</li> <li>If the target exists, the slice will be appended. Check if the slice to be    appended is last. If not, refuse to append (alternative: insert but this is    probably difficult or error prone).</li> <li>Slices are appended in the order they are provided.</li> <li>If a slice is not yet available, wait and retry until it <ul> <li>exists, and</li> <li>is complete.</li> </ul> </li> <li>Check for each slice that it is valid. A valid slice<ul> <li>is self-consistent, </li> <li>has the same structure as target, and</li> <li>has an append dimension whose size is equal to the target chunking of   this dimension.</li> </ul> </li> <li>Before appending a slice, lock the target so that another tool invocation    can recognize it, e.g., write a lock file.</li> <li>If the target is locked, either wait until it becomes available or exit    with an error. The behaviour is controlled by a tool option.</li> <li>After successfully appending a slice, remove the lock from the target.</li> <li>Appending a slice shall be an atomic operation to ensure target dataset    integrity. That is, in case a former append step failed, a rollback must   be performed to restore the last valid state of the target. Rolling back   shall take place after an append failed, or before a new slice is appended,   or to sanitize a target to make it usable again. Rolling back shall    include restoring all changed files, removing all added files,    and removing any locks. </li> <li>The tool shall allow for continuing appending slices at the point   it failed.</li> <li>The tool shall offer a CLI and a Python API.<ul> <li>Using the CLI, slices are given as a variadic argument that provides the    file paths into the slice filesystem.</li> <li>Using the Python API, it shall be possible to provide the slices by    specifying a function that generates the slice datasets and an   iterable providing the arguments for the function.   This is similar how the Python <code>map()</code> built-in works.</li> </ul> </li> </ul>"},{"location":"requirements/#further-ideas","title":"Further Ideas","text":"<ul> <li>Allow for inserting and deleting slices.</li> <li>Allow for updating slices.   #29</li> <li>Allow specifying a constant delta between coordinates of the append dimension.   and verify coordinate deltas of append dimension to be constant.    #21</li> <li>Verify append dimension coordinates increase or decrease monotonically.    #20</li> <li>Integration with xcube:<ul> <li>Add xcube server API: Add endpoint to xcube server that works similar    to the CLI and also uses a similar request parameters.</li> <li>Use it in xcube data stores for the <code>write_data()</code> method, as a parameter    to enforce sequential writing of Zarr datasets as a robust option when a    plain write fails.</li> </ul> </li> </ul>"},{"location":"start/","title":"Getting Started","text":""},{"location":"start/#installation","title":"Installation","text":"<p><code>zappend</code> requires a Python v3.10+ environment. To install the latest released version from PyPI:</p> <pre><code>pip install zappend\n</code></pre> <p>To install the latest version for development, clone the repository, and with the repository\u2019s root directory as the current working directory execute:</p> <pre><code>pip install --editable .\n</code></pre>"},{"location":"start/#using-the-cli","title":"Using the CLI","text":"<p>Get usage help:</p> <pre><code>zappend --help\n</code></pre> <p>Get configuration help in Markdown format (json also available):</p> <pre><code>zappend --help-config md\n</code></pre> <p>Process list of local slice paths:</p> <pre><code>zappend --target target.zarr slice-1.nc slice-2.nc slice-3.nc\n</code></pre> <p>Process list of local slice paths with configuration in <code>config.yaml</code>:</p> <pre><code>zappend --config config.yaml slice-1.nc slice-2.nc slice-3.nc\n</code></pre>"},{"location":"start/#using-the-python-api","title":"Using the Python API","text":"<p>Process list of local slice paths:</p> <pre><code>from zappend.api import zappend\n\nzappend([\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"], target_dir=\"target.zarr\")\n</code></pre> <p>Process list of slices stored in S3 configuration in <code>config</code>:</p> <pre><code>from zappend.api import zappend\n\nconfig = { \n    \"target_dir\": \"target.zarr\",\n    \"slice_storage_options\": {\n        \"key\": \"...\",               \n        \"secret\": \"...\",               \n    } \n}\n\nzappend((f\"s3:/mybucket/data/{name}\" \n         for name in [\"slice-1.nc\", \"slice-2.nc\", \"slice-3.nc\"]), \n        config=config)\n</code></pre> <p>Slice datasets can be passed in a number of ways; please refer to the section  Slice Sources in the User Guide.</p>"}]}